{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170aa691-2972-41f0-8b62-7e15825b706a",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Books/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163e9f0-8ae7-4035-917f-a6d6684e12da",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - Modeling](18.02-mlpg-Other-Considerations-Modeling.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Algorithm Comparisons](18.04-mlpg-Other-Considerations-Algorithm-Comparisons.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345a03f-a072-46c1-a966-b3f59f3864a8",
   "metadata": {},
   "source": [
    "# 18. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5b384-e97d-41e7-bb2b-9fe83cd45e98",
   "metadata": {},
   "source": [
    "## 18.3. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7b5f2-247d-48ed-93cb-4fb5d6cc60f9",
   "metadata": {},
   "source": [
    "### 18.3.1. Multinomial Logistic Regression\n",
    "* **Binomial Logistic Regression**: Standard logistic regression that predicts a binomial probability (i.e. for two classes) for each input example\n",
    "* **Multinomial Logistic Regression**: Modified version of logistic regression that predicts a multinomial probability (i.e. more than two classes – multi-class classification problem) for each input example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486ed44-8151-4703-b206-925282b3767c",
   "metadata": {},
   "source": [
    "### 18.3.2. Robust Regression\n",
    "* **Robust regression** refers to a suite of algorithms that are robust to outliers present in training data\n",
    "* Robust regression algorithms can be used for data with outliers in the input or target values\n",
    "* Examples of Robust regression algorithms are:\n",
    "  - Huber Regression\n",
    "  - RANSAC Regression\n",
    "  - Theil Sen Regression\n",
    "\n",
    "**Comparison of Robust Regression algorithms Line of Best Fit:**<br>\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-RobustRegression.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [ (Source) ](https://machinelearningmastery.com/robust-regression-for-machine-learning-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda56ac-28e3-4b71-88c9-5376ea1e7a89",
   "metadata": {},
   "source": [
    "### 18.3.3. Linear Discriminant Analysis (LDA)\n",
    "* Linear Discriminant Analysis, or LDA for short, is a classification machine learning algorithm\n",
    "* LDA assumes that the input variables are numeric and normally distributed and that they have the same variance (spread). If this is not the case, it may be desirable to transform the data to have a Gaussian distribution and standardize or normalize the data before modeling\n",
    "* It also assumes that the input variables are not correlated; if they are, a PCA transform may be helpful to remove the linear dependence\n",
    "* The LDA model is naturally multi-class. This means that it supports two-class classification problems and extends to more than two classes (multi-class classification) without modification or augmentation\n",
    "* It is a linear classification algorithm, like logistic regression. This means that classes are separated in the feature space by lines or hyperplanes. Extensions of the method can be used that allow other shapes, like Quadratic Discriminant Analysis (QDA), which allows curved shapes in the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113697b-e09e-4844-9afd-5d851a183d14",
   "metadata": {},
   "source": [
    "### 18.3.4. Nearest Radius Neighbors (NRN) algorithm\n",
    "* The NRN Classifier is a simple extension of the k-nearest neighbors classification algorithm\n",
    "* It is based on the k-nearest neighbors algorithm or kNN. kNN involves taking the entire training dataset and storing it. Then, at prediction time, the k-closest examples in the training dataset are located for each new example for which we want to predict. The mode (most common value) class label from the k neighbors is then assigned to the new example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb085d-ed25-43a9-9005-f818c110fc7a",
   "metadata": {},
   "source": [
    "### 18.3.5. Gaussian Processes Classification (GPC) algorithm\n",
    "* The GPC is a non-parametric algorithm that can be applied to binary classification tasks\n",
    "* A Gaussian process is a generalization of the Gaussian probability distribution\n",
    "* Gaussian processes can be used as a machine learning algorithm for classification predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960d18d-c40a-49c1-b156-99621922a503",
   "metadata": {},
   "source": [
    "### 18.3.6. Clustering algorithms (Unsupervised)\n",
    "* Clustering is an unsupervised problem of finding natural groups in the feature space of input data\n",
    "* It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior, so-called pattern discovery, or knowledge discovery\n",
    "* Examples of cluster problems:\n",
    "  - Market segmentation\n",
    "  - Separating normal data from outliers or anomalies \n",
    "  - Clustering can also be useful as a type of feature engineering, where existing and new examples can be mapped and labeled as belonging to one of the identified clusters in the data\n",
    "* Many algorithms use similarity or distance measures between examples in the feature space to discover dense regions of observations\n",
    "* As such, it is often good practice to scale data before using clustering algorithms\n",
    "* Cluster analysis is an iterative process where a subjective evaluation of the identified clusters is fed back into changes to algorithm configuration until a desired or appropriate result is achieved\n",
    "* 10 popular clustering algorithms:<br>\n",
    "  - `1) Affinity Propagation`\n",
    "    - Finding a set of exemplars that best summarize the data\n",
    "  - `2) Agglomerative Clustering`\n",
    "    - Merging examples until the desired number of clusters is achieved\n",
    "  - `3) BIRCH`\n",
    "    - Constructing a tree structure from which cluster centroids are extracted\n",
    "  - `4) DBSCAN`\n",
    "    - Finding high-density areas in the domain and expanding those areas of the feature space around them as clusters\n",
    "  - `5) K-Means`\n",
    "    - Assigning examples to clusters to minimize the variance within each cluster\n",
    "  - `6) Mini-Batch K-Means`\n",
    "    - A modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise\n",
    "  - `7) Mean Shift`\n",
    "    - Finding and adapting centroids based on the density of examples in the feature space\n",
    "  - `8) OPTICS`\n",
    "    - A modified version of DBSCAN\n",
    "  - `9) Spectral Clustering`\n",
    "    - A general class of clustering methods, drawn from linear algebra\n",
    "  - `10) Mixture of Gaussians`\n",
    "    - Summarizes a multivariate probability density function with a mixture of Gaussian probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3c0e2-6f1b-410d-b529-d9ead56a598e",
   "metadata": {},
   "source": [
    "### 18.3.7. Working of an unsupervised learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360674d1-c56a-4a0d-b0cc-1bdfdd43fbd3",
   "metadata": {},
   "source": [
    "**Introduction to Unsupervised Learning:**\n",
    "* Unsupervised learning is contrasted with Supervised learning because it uses an **unlabeled** training dataset rather than a labeled one, in other words, we don't have the vector of expected results, we only have a dataset of features where we can find structure\n",
    "* Clustering is good for:\n",
    "  - Market segmentation\n",
    "  - Social network analysis\n",
    "  - Organizing computer clusters\n",
    "  - Astronomical data analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a59adb-9d68-4f6d-a263-ccb9dc9af459",
   "metadata": {},
   "source": [
    "**K-Means Clustering algorithm:**\n",
    "* It is the most commonly used algorithm for automatically grouping data into coherent subsets and it works as follows:\n",
    "  - **Step 1**: `Initialize`\n",
    "    - Randomly initialize the **K** number of _`cluster centroids`_ (data points in the dataset)\n",
    "  - **Step 2**: `Cluster assignment`\n",
    "    - Assign all examples into one or more groups based on which cluster centroid the example is closest to\n",
    "  - **Step 3**: `Move centroid`\n",
    "    - Compute the averages for all the points inside each of the centroid groups, then,\n",
    "    - Move the cluster centroid points to those averages\n",
    "  - **Step 4**: `Repeat`\n",
    "    - Re-run steps 2 & 3 until all the clusters are found\n",
    "* The main variables/parameters are:\n",
    "  - **K** (_`number of clusters`_)\n",
    "  - **X** (_`Training dataset`_ [$x^1, x^2,…,x^m$])\n",
    "* If we have a cluster centroid with 0 points assigned to it, we can `randomly re-initialize` that centroid to a new point, or, simply `eliminate` that cluster group\n",
    "* After several iterations, the algorithm will converge where new iterations do not affect the clusters\n",
    "* Non-separated clusters: \n",
    "  - Some datasets have no real inner separation or natural structure\n",
    "  - K-Means algorithm can still evenly segment the data into **K** subsets so can still be useful\n",
    "* **`Choosing the Number of Clusters`** (i.e., **`Step 1`**):\n",
    "  - Choosing K can be quite arbitrary and ambiguous\n",
    "  - **Elbow method:** \n",
    "    - Plot the cost function **J** and the number of clusters **K**\n",
    "    - The **J** should reduce as we increase **K**, then flatten out\n",
    "    - Choose **K** at the point where the cost **J** starts to flatten out\n",
    "    - However, fairly often, the **curve is very gradual**, so there is **no clear elbow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd1dbe-41e4-4ebc-9811-b4331a4f300c",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "* The cost function **J** always decreases as **K** is increased\n",
    "* There is one exception when K-Means gets stuck at a bad local optimum\n",
    "* Another way to choose **K** is to observe how well K-Means performs on a `downstream purpose`, in other words, we choose K that proves to be most useful for some goals we are trying to achieve from clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadd6ca-bac2-41ce-9f22-fcf4732619fa",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-ChoosingKValue1.png\"><br>\n",
    "<br><br><br><br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)\n",
    "\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-ChoosingKValue2.png\"><br>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c22d68-70e6-4810-9734-1d50d211c844",
   "metadata": {},
   "source": [
    "### 18.3.8. Limitations of k-Means clustering\n",
    "* Works well for simple clusters that are the same size, well-separated, globular shapes\n",
    "* But, does not do well with irregular, complex clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62c220-8b7d-403b-baca-6038ffae4f6b",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Other Considerations - Modeling](18.02-mlpg-Other-Considerations-Modeling.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Algorithm Comparisons](18.04-mlpg-Other-Considerations-Algorithm-Comparisons.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
