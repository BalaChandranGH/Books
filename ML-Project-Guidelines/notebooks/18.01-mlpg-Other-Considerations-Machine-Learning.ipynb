{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36f4fcb-9542-4ae6-9419-5afd2aae21d8",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Whitepapers/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b6351-c01f-4f7e-86c1-2e3a069e4027",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Stage-11: Model Deployment](17.00-mlpg-Stage-11-Model-Deployment.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Modeling](18.02-mlpg-Other-Considerations-Modeling.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886a76e-b7ed-4251-be90-60715813d045",
   "metadata": {},
   "source": [
    "# 18. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2781246-a8d6-4853-80bf-9ee815108f24",
   "metadata": {},
   "source": [
    "## 18.1. Machine Larning\n",
    "### 18.1.1. No Free Lunch Theorem for Machine Learning\n",
    "* The No Free Lunch Theorem, often abbreviated as NFL or NFLT, is a theoretical finding that suggests all optimization algorithms perform equally well when their performance is averaged over all possible objective functions\n",
    "* It implies that there is no single best optimization algorithm. Because of the close [relationship between optimization, search, and machine learning](https://machinelearningmastery.com/applied-machine-learning-as-a-search-problem/), it also implies that there is no single best machine learning algorithm for predictive modeling problems such as classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a18ee-34e0-4960-9d48-7771b85628ad",
   "metadata": {},
   "source": [
    "### 18.1.2. Ensemble Learning\n",
    "* An ensemble is a machine learning model that combines the predictions from two or more models\n",
    "* Why should we consider using an ensemble?\n",
    "  - **Performance**: An ensemble can make better predictions and achieve better performance than any single contributing model\n",
    "  - **Robustness**: An ensemble reduces the spread or dispersion of the predictions and model performance. In other words, it’s the **reliability** of the average performance of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2428f-cc81-492a-a2fb-1d360a5ae6b3",
   "metadata": {},
   "source": [
    "### 18.1.3. How Do Ensembles Work\n",
    "**Intuition for Regression Ensembles:**<br>\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-IntuitionRegEnsembles.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [ (Source) ](https://machinelearningmastery.com/how-ensemble-learning-works/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006b071-1e6c-488f-b19b-05ac955724c7",
   "metadata": {},
   "source": [
    "**Intuition for Classification Ensembles:**<br>\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-IntuitionClassEnsembles.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [ (Source) ](https://machinelearningmastery.com/how-ensemble-learning-works/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba0ece-18f6-4e6f-9b9d-d7320798890f",
   "metadata": {},
   "source": [
    "### 18.1.4. Machine Learning workflow\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-MLWorkflow.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1970d-af2a-4f61-a30c-e4ff78c18298",
   "metadata": {},
   "source": [
    "### 18.1.5. Types of Unsupervised Learning\n",
    "* USL involves tasks that operate on datasets without labeled responses or target values\n",
    "* Instead, the goal is to capture interesting structure or information\n",
    "* **Applications** of unsupervised learning:\n",
    "  - Visualize the structure of a complex dataset\n",
    "  - Density estimation to predict probabilities of events\n",
    "  - Compress and summarize data\n",
    "  - Extract features for supervised learning\n",
    "  - Discover important clusters or outliers\n",
    "* **Two major types** of unsupervised learning methods:\n",
    "  - **Transformations**: Process that extract or compute information\n",
    "  - **Clustering**: Find groups in data and assign every point in the dataset to one of the groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38866-2d2e-4de6-b17d-be9660763997",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-USLMethodTransformations.png\"><br>\n",
    "<br><br><br><br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169bdb8-f750-4fe4-8859-4a1820a459a4",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-USLMethodClustering.png\"><br>\n",
    "<br><br><br><br>\n",
    "Image credit [ (Source) ](https://www.coursera.org/in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f9ca9-3674-4e4a-9faa-d158c6984de9",
   "metadata": {},
   "source": [
    "### 18.1.6. Hard Clustering and Soft (or Fuzzy) Clustering\n",
    "* **Hard clustering**: each data point belongs to exactly one cluster\n",
    "* **Soft clustering**: each data point is assigned a weight/score/probability of membership for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20213464-1afd-4bbc-89e2-42005a85b6be",
   "metadata": {},
   "source": [
    "## 18.1.7.. How to choose ML Algorithms?\n",
    "* No free lunch theorem:\n",
    "  - “All algorithms perform equally well when their performances are averaged over all possible objective functions”\n",
    "  - There is no single best algorithm for predictive modeling problems such as regression and classification\n",
    "* Determining which algorithm to use depends on many factors from the ``type of problem`` at hand to the ``type of output`` we are looking for\n",
    "* The algorithm selection depends on the following factors, to name a few:\n",
    "  1. Business objective or Problem statement (both in business terms and analytical terms)\n",
    "  2. Type of the data\n",
    "  3. Size of the data\n",
    "  4. Feature space (number of features)\n",
    "  5. Linearity \n",
    "  6. Accuracy and/or Interpretability of the output\n",
    "  7. Algorithm run-time or Training time\n",
    "  8. Available computing power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17b574-e5ae-47cb-8d39-816b9ca7c4a4",
   "metadata": {},
   "source": [
    "### 18.1.7.1. Defining Business objective / Problem statement\n",
    "* Define the problem statement both in business terms and analytical terms\n",
    "* Use the following guideline to select the algorithms:\n",
    "  - ``Predict numerical values`` such as forecast: ``REGRESSION`` problem\n",
    "    - Linear – Low computing power, Fast training time<br>\n",
    "      _LinearRegression, LASSO, Ridge, ElasticNet_\n",
    "    - Non-linear – High computing power, Long training time, More memory<br>\n",
    "      _KNN, SVM, Decision Trees, Neural Networks_\n",
    "    - Ensembles (Bagging & Boosting) – Accurate, Fast training time, More memory <br>\n",
    "      _Random Forest, Extreme Boosting_\n",
    "  - ``Predict categories/classes`` (both binary class & multi-class): ``CLASSIFICATION`` problem\n",
    "    - Logistic Regression – Fast training time\n",
    "    - Neural Network – Accurate, Long training time\n",
    "    - Random Forest – Accurate, Fast training time, More memory\n",
    "    - SVM – Long training time, More memory, use under 100 features\n",
    "  - ``Discover structure``: ``CLUSTERING`` problem\n",
    "    - K-Means\n",
    "  - ``Find unusual occurrences``: ``Anomaly Detection``\n",
    "    - One-class SVM – use under 100 features\n",
    "    - Local Outlier Factor (neighbors-based technique)\n",
    "    - Minimum Covariance Determinant \n",
    "    - Isolation Forest (ensemble technique)\n",
    "  - ``Generate Recommendations``: ``Recommender System``\n",
    "    - Content-based filtering (Supervised learning - Classification):\n",
    "       - Recommends products that are similar to the ones that a user has liked in the past\n",
    "    - Collaborative Filtering (Supervised learning - Classification):\n",
    "       - Recommends products based on the user behavior that is similar to other user groups\n",
    "       - Simple and appropriate for small datasets\n",
    "    - Clustering (Unsupervised Learning):\n",
    "       - Appropriate for large datasets\n",
    "    - Neural Networks\n",
    "       - Deep learning approach used by YouTube\n",
    "  - ``Information Extraction``: ``Natural Language Processing (NLP)``\n",
    "    - NLTK (Natural Language Toolkit)\n",
    "  - ``Networks and Graphs analysis``:\n",
    "    - NetworkX\n",
    "  - ``Geographic/Geospatial/Maps``:\n",
    "    - Folium\n",
    "  - ``Image and video processing``:\n",
    "    - OpenCV\n",
    "    - TensorFlow (deep learning platform)\n",
    "    - Keras (Python API built on top of TensorFlow)\n",
    "    - PyTorch (deep learning library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc88e0-6ac3-4f8a-b547-0a12974b660a",
   "metadata": {},
   "source": [
    "### 18.1.7.2. Type of data\n",
    "* Datasets may contain only numerical values or categorical values or texts or images or audios or videos or a mixture of any/all of these\n",
    "* The type(s) of the data play an important role in branching out the options for algorithm types such as Regression, Classification, Clustering, Text extraction, Maps, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c0b66b-2a98-4027-aecb-1d66eefb52ed",
   "metadata": {},
   "source": [
    "### 18.1.7.3. Size of the data\n",
    "* It’s recommended to gather a good amount of data to get reliable predictions, however, many a time, the availability of data is a constraint\n",
    "* ``If the training data is smaller`` or it has a fewer # of observations and a higher # of features, then ``choose algorithms with high bias/low variance like Linear regression, Naïve Bayes, or Linear SVM``\n",
    "* ``If the training data is sufficiently large`` and the # of observations is higher as compared to the # of features, then, ``choose low bias/high variance algorithms like KNN, Decision trees, or kernel SVM``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826fbb2-4921-431f-aa87-50e42c2f04da",
   "metadata": {},
   "source": [
    "### 18.1.7.4. Feature space (number of features)\n",
    "* The dataset may have a large number of features that may not all be relevant and significant\n",
    "* For a certain type of data, such as genetics or textual, the number of features can be very large compared to the number of data points\n",
    "* A large number of features can bog down some algorithms, making training time unfeasibly long\n",
    "* ``If the feature space is large and lesser observations``, then ``use Linear SVM``\n",
    "* Use PCA and feature selection techniques to reduce dimensionality and select important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad468b4b-f347-4494-82d0-e325c9c3d30c",
   "metadata": {},
   "source": [
    "### 18.1.7.5. Linearity\n",
    "* Many algorithms work on the assumption that classes can be separated by a straight line, e.g., Logistic Regression and SVMs\n",
    "* Linear Regression algorithms assume that data trends follow a straight line, If the data is linear, then these algorithms perform quite good\n",
    "* However, not always is the data is linear, so we require other algorithms which can handle high dimensional and complex data structures, e.g., kernel SVM, Random Forest, NNs\n",
    "* ``The best way to find out the linearity is to either fit a linear line or run a Logistic Regression or Linear SVM and check for residual errors. A higher error means the data is not linear and would need complex algorithms to fit``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72eab7d-418a-4eef-84e7-9f53080e2ece",
   "metadata": {},
   "source": [
    "### 18.1.7.6. Accuracy/Interpretability of the output\n",
    "* ``Accuracy``: A function that predicts a response value for a given observation, which is close to the true response value for that observation\n",
    "* ``Flexible models`` give ``high accuracy`` but ``low interpretability`` (e.g., KNN with k=1)\n",
    "* ``Restrictive models`` give ``low accuracy`` but ``high interpretability`` (e.g., linear regression)\n",
    "* The choice of the algorithm depends on the objective of the business problem\n",
    "* If the ``inference is the goal``, then ``Restrictive models are better`` as they are much more interpretable\n",
    "* ``If accuracy is the goal``, then ``Flexible models are better``\n",
    "* In general, as the flexibility of a method increases, its interpretability decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e13db-35c5-4313-8f03-7062c1c194a2",
   "metadata": {},
   "source": [
    "**Trade-off between _``Accuracy``_ and _``Interpretability``_**<br>\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-AccuracyVsInterpretability.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [(Source)](https://cdn.oreillystatic.com/en/assets/1/event/105/Overcoming%20the%20Barriers%20to%20Production-Ready%20Machine-Learning%20Workflows%20Presentation%201.pdf)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b997f2-f01f-4f03-a370-b59335c309ba",
   "metadata": {},
   "source": [
    "<br>**Trade-off between _``Flexibility``_ and _``Interpretability``_**<br>\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-FlexibilityVsInterpretability.png\"><br>\n",
    "<br><br><br><br><br>\n",
    "Image credit [(Source)](https://faculty.marshall.usc.edu/gareth-james/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55183-7226-4387-bd45-0f46640ce675",
   "metadata": {},
   "source": [
    "### 18.1.7.7. Algorithm run-time/Training time\n",
    "* Higher accuracy typically means higher training time\n",
    "* Also, algorithms require more time to train on large training data\n",
    "* In real-world applications, the choice of algorithm is driven by these two factors predominantly\n",
    "* ``Short run-time and easy to implement but low accuracy: Naïve Bayes and Linear and Logistic regression``\n",
    "* ``Long run-time and not so easy to implement but high accuracy: Random Forest, NN, SVM``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c245be-218b-4c17-8411-1d78c5d0af1a",
   "metadata": {},
   "source": [
    "### 18.1.7.8. Available computing power\n",
    "* The choice of algorithms is based on the available CPUs and GPUs as well\n",
    "* If multiple CPUs and GPUs are available with multiple cores, then deep learning can be a good option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fed15f-bf4d-4eb2-96f2-3d7f5b484af6",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-MLAlgorithmsCheatSheet1.png\"><br>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "Image credit [(Source)](https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d20ee1-4a4f-4176-b3dc-cca9d613a5c3",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-MLAlgorithmsCheatSheet2.png\"><br>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "Image credit [(Source)](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d888026-0fdc-4603-9428-a72f4500c160",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-OC-MLAlgorithmsCheatSheet3.png\"><br>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "Image credit [(Source)](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f98561-7b09-4f47-b2b7-ee40f81654eb",
   "metadata": {},
   "source": [
    "### 18.1.7.9. Rule of Thumb\n",
    "* **``It's not who has the best algorithm that wins. It's who has the most data``** – (Andrew Ng)\n",
    "* **``An inferior algorithm with enough data can outperform a superior algorithm with less data``** – (A. Ng)\n",
    "* **``More data beats better algorithms but better data beats more data``** – (Bala)<br>\n",
    "  **Step1**: Define the problem in analytical terms from the business objectives<br>\n",
    "  **Step2**: Start with a simple algorithm, build a baseline model, benchmark it, and be familiar with the data<br>\n",
    "  **Step3**: Then try with more complex algorithms and build complex models to meet the business needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d47a88-0396-44c5-bbfe-21157a8224ac",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Stage-11: Model Deployment](17.00-mlpg-Stage-11-Model-Deployment.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Modeling](18.02-mlpg-Other-Considerations-Modeling.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
