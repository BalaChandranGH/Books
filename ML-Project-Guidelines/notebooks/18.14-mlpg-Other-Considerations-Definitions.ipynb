{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f8d3e3-7f93-4ef8-8c94-7fc9296815b5",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Books/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64afb8-7013-49c8-8ac3-f0111963f078",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - Deep Learning](18.13-mlpg-Other-Considerations-Deep-Learning.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Miscellaneous](18.15-mlpg-Other-Considerations-Miscellaneous.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e0e4f-4366-40e9-9ca5-3489e3debdd0",
   "metadata": {},
   "source": [
    "# 18. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd2f2e-db27-46ee-ba08-160e3e3297ca",
   "metadata": {},
   "source": [
    "## 18.14. Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84838c0-3641-48aa-935e-a5743baa8971",
   "metadata": {},
   "source": [
    "### 18.14.1. Optimization\n",
    "* Optimization refers to finding the set of inputs to an objective function that results in the maximum or minimum output from the objective function\n",
    "* It is common to describe optimization problems in terms of **local vs. global optimization**\n",
    "* Similarly, it is also common to describe optimization algorithms or search algorithms in terms of **local vs. global search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa10c34-f913-4007-b7b9-a643c820dd77",
   "metadata": {},
   "source": [
    "### 18.14.2. Linear Regression with One Variable\n",
    "* Linear Regression with One Variable is also known as **Univariate Linear Regression (ULR)**\n",
    "* ULR is used when we want to predict a single output value (y) from a single input value (x)\n",
    "* Univariate Hypothesis function (i.e., equation of a straight line):<br>\n",
    "&emsp; $\\hat y = h{_\\theta}(x) = \\theta{_0} + \\theta{_1} x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ce697-a323-43b9-9337-093496a46080",
   "metadata": {},
   "source": [
    "### 18.14.3. Cost function (CF)\n",
    "* CF is also known as **Squared Error Function** or **Mean Squared Function**\n",
    "* The CF is used to measure the accuracy of our hypothesis function\n",
    "* The goal is to minimize the cost function to reach the global minimum:<br>\n",
    "&emsp; $J(\\theta_{0},\\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\hat y_{i} - y_{i})^2 = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x_i) - y_{i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefd878-1e6f-4422-b240-0e111d7dba7c",
   "metadata": {},
   "source": [
    "### 18.14.4. Gradient Descent (GD)\n",
    "* GD is an algorithm that repeats until convergence (i.e., until it reaches the global optimum), using the cost function\n",
    "* The step size can be controlled via hyperparameters (e.g., alpha):<br>\n",
    "&emsp; **repeat until convergence: {**<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_{i}) - y_{i})$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}((h_{\\theta}(x_{i}) - y_{i})x_{i})$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; **}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac39a0-4643-4b0d-b521-f1f53ca5d96b",
   "metadata": {},
   "source": [
    "### 18.14.5. Matrix, Vector, and Vectorization\n",
    "* A Matrix is a 2D array, with m rows and n columns (m x n)\n",
    "* A Vector is a matrix with m rows and only one column (m x 1). 4x1 is a 4-dimensional vector\n",
    "* Vectorization helps to achieve parallel processing through matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad4850-2f1a-4ad8-a3bb-ceabb72d8d08",
   "metadata": {},
   "source": [
    "### 18.14.6. Linear Regression with Multiple Variables\n",
    "* Linear Regression with Multiple Variables is also known as **Multivariate Linear Regression (MLR)**\n",
    "* MLR is used when we want to predict a single output value (y) from many input values (x1,…,xn)\n",
    "* Multivariate Hypothesis function:<br>\n",
    "&emsp; $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} \\ +\\ ... + \\ \\theta_{n}x_{n}$\n",
    "\n",
    "&emsp;&emsp;&emsp; The vectorized implementation of the above hypothesis function is:<br>\n",
    "&emsp;&emsp;&emsp; $h_{\\theta}(x) = [\\theta_{0} \\ \\ \\ \\theta_{1} \\ \\ \\ ... \\ \\ \\ \\theta_{n}] = \\left[\\begin{array}{cc}x_{0}\\\\ x_{1} \\\\.\\\\.\\\\.\\\\ x_{n}\\\\ \\end{array}\\right]  = \\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909290a2-04b2-4204-96be-384ff8307c90",
   "metadata": {},
   "source": [
    "### 18.14.7. Normal Equation (NE)\n",
    "* The NE is a method of finding the optimum theta value without iterations\n",
    "* There is no need to do Feature Scaling with the normal equation<br>\n",
    "&emsp; $\\theta = (X^TX)^{-1}X^Ty$\n",
    "* The NE works well with a small-medium number of features, however, if the number of features exceeds 10,000, it would be a good time to switch to iterations (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f91c1-caa8-491e-acb9-fd2121440951",
   "metadata": {},
   "source": [
    "### 18.14.8. Backpropagation algorithm\n",
    "* Backpropagation is a NN terminology for minimizing the cost function, just like what’s done with gradient descent in linear and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62496ab1-7f2f-4828-a0a3-e16dc1cca697",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - Deep Learning](18.13-mlpg-Other-Considerations-Deep-Learning.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Miscellaneous](18.15-mlpg-Other-Considerations-Miscellaneous.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
