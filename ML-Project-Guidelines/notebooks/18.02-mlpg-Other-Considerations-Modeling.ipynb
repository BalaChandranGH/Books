{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fe27cc-85a4-4da9-947f-54e55e9bb21c",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Books/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32461649-5f7d-499e-aab1-c64aef5968e0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - Machine-Learning](18.01-mlpg-Other-Considerations-Machine-Learning.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Algorithms](18.03-mlpg-Other-Considerations-Algorithms.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658f383-754f-495a-b2e5-04101eb663f7",
   "metadata": {},
   "source": [
    "# 18. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9f9ce-fe13-416e-9482-b877ebc67dbb",
   "metadata": {},
   "source": [
    "## 18.2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f26d3-3883-4064-80b3-33c4e4123c3c",
   "metadata": {},
   "source": [
    "### 18.2.1. Most commonly used model categories in ML\n",
    "* **Predictive models** (Supervised Learning): \n",
    "  - Analyze the past performance to predict future predictions\n",
    "  - There are 3 types:\n",
    "    - Regression models \n",
    "    - Classification models\n",
    "    - Time-series models\n",
    "* **Descriptive models** (Unsupervised Learning): \n",
    "  - Quantify the relationships in data in a way that is often used to classify data sets into groups\n",
    "  - There are 2 types:\n",
    "    - Clustering (e.g., K-Means)\n",
    "    - Association rules (e.g., Apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2a443-eca1-46ec-a5d5-cf4ae65e3158",
   "metadata": {},
   "source": [
    "### 18.2.2. The drawbacks of a linear model\n",
    "* Sensitive to outliers\n",
    "* Overfitting – if the number of observations is much higher compared to the number of features \n",
    "* Assumes linear relationships between features; any nonlinear relationship that exists would result in a bad model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f910b-2c05-4ceb-91de-1619479856f4",
   "metadata": {},
   "source": [
    "### 18.2.3. Cross-Validation\n",
    "* CV is a statistical method used to estimate the performance of ML models on unseen data\n",
    "* CV is a resampling or out-of-sampling method\n",
    "* Used to protect against overfitting in a predictive model, in a case where the amount of data is limited \n",
    "* CV is performed on the training dataset (e.g., k-fold cv, stratified k-fold cv, etc.)\n",
    "* CV is a method that goes beyond evaluating a single model using a single Train/Test split of the data; it uses multiple Train/Test splits, each of which is used to train and evaluate a separate model\n",
    "* CV is used to evaluate the model and not learn or tune a new model\n",
    "* When applying Supervised learning methods, follow a consistent series of steps:\n",
    "  1) _`Partition the data set`_ into training and test sets using the Train/Test split function\n",
    "  2) _`Call the Fit Method`_ on the training set to estimate the model\n",
    "  3) _`Apply the model by using the Predict Method`_ to estimate a target value for the new data instances, (or) by using the Score Method to evaluate the trained model's performance on the test set\n",
    "* To do model tuning, see how to tune the model parameters using something called \"Grid Search\"\n",
    "* A note on performing CV for more advanced scenarios:\n",
    "  - In some cases (e.g. when feature values have very different ranges), we need to scale/ normalize the training & test sets before use with a classifier. The proper way to do CV when you need to scale the data is not to scale the entire dataset with a single transform, since this will indirectly leak info into the training data about the whole dataset, including the test data (data leakage). Instead, scaling/normalizing must be computed and applied for each CV fold separately. To do this, the easiest way in scikit-learn is to use `pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3f656-073d-4e93-a0ba-531ad765732d",
   "metadata": {},
   "source": [
    "### 18.2.4. Differences between PCA and EFA\n",
    "![](figures/MLPG-OC-DifferencesPCAEFA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e4469-c71c-4f21-bf67-992dcbe31054",
   "metadata": {},
   "source": [
    "### 18.2.5. Dimensionality Reduction and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd2ab2-4d69-41d8-8f82-e87c32395068",
   "metadata": {},
   "source": [
    "**Dimensionality reduction (DR):**\n",
    "* DR is the transformation of input features from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains meaningful properties of the original data\n",
    "* DR refers to techniques that reduce the number of input variables/features in a dataset\n",
    "* DR is a data preparation technique performed on data before modeling, and It might be performed after data cleaning and data scaling and before training a predictive model\n",
    "* `DR methods` include `PCA, feature selection/engineering, matrix factorization, autoencoders`, etc.\n",
    "* The number of input variables/features for a dataset is referred to as its dimensionality\n",
    "* Higher dimensionality may mean 100s, 1000s, or even millions of input variables\n",
    "* More input features often make a predictive modeling task more challenging to model, more generally referred to as the `curse of dimensionality`\n",
    "* There is no best technique for DR and no mapping of techniques to problems\n",
    "* It is good practice to either normalize or standardize data before using these methods if the input variables have different scales or units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c085250-544a-486b-8e9e-6962ddd3a8a9",
   "metadata": {},
   "source": [
    "**Why do we need DR techniques** (or) **the motivations for using them:**\n",
    "* **Data compression:**\n",
    "  - Less data means `less computer memory` which will `speed up the learning algorithm`\n",
    "  - **Note**: DR techniques reduce the **`n`** (`number of features`); not **`m`** (`number of observations which will remain the same`)\n",
    "* **Visualization:**\n",
    "  - It’s not easy to visualize data that is more than 3Ds\n",
    "  - Reduce the dimensions of the data to 3 or less to plot it\n",
    "  - We need to find a few features that can `summarize` all other features\n",
    "  - `Example`: 100s of features related to a country’s economic system may be combined into 1 feature and that can be called “Economic activity”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6eafb0-0dc7-48d9-be04-1c010fafdb46",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA):**\n",
    "* PCA is one of the most commonly used dimensionality reduction technique\n",
    "* It’s used to speed up supervised learning (both regression and classification)\n",
    "* Any DR (such as PCA) performed on a training set must also be performed on new data, such as a test set, validation set, and data when making a prediction with the final model, however, note:\n",
    "  - Define the PCA reduction ($x^{(i)}$ to $z^{(i)}$) only on the training set; not on the CV or test sets\n",
    "  - Apply the mapping of $z^{(i)}$ to the CV and test sets after it is defined on the training set\n",
    "* **Rule of Thumb:**\n",
    "  - `Do not assume we need to do PCA; try the full ML algorithm first, then use PCA if needed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb3f77-2f00-4626-b28c-4742bb6f4c96",
   "metadata": {},
   "source": [
    "**Purpose of PCA:**\n",
    "* Often used as a dimensionality-reduction technique to increase the interpretability of the datasets\n",
    "* Represents multivariate data as a smaller set of variables (summary indices) to observe trends, jumps, clusters, and outliers\n",
    "* Particularly useful in processing data where multi-colinearity exists between the features\n",
    "* Used to explain the variance-covariance structure of a set of variables through linear combinations\n",
    "* The projection of data in a smaller feature space (normally 2D) acts as an overview and may uncover the relationships between observations and variables, and among the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5523c-38a6-4abd-b03e-0b87f2988026",
   "metadata": {},
   "source": [
    "**Bad use of PCA:**\n",
    "* Trying to prevent overfitting\n",
    "  - We may think that reducing the feature space with PCA would address overfitting\n",
    "  - It might work sometimes, but is not recommended because it does not consider the values of the results y\n",
    "  - It’s better to use regularization instead of PCA to address overfitting issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2be47-58ea-41ee-8349-833f52553a15",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Other Considerations - Machine-Learning](18.01-mlpg-Other-Considerations-Machine-Learning.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Algorithms](18.03-mlpg-Other-Considerations-Algorithms.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
