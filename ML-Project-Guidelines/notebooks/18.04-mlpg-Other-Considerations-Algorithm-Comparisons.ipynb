{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6a058c-3600-4ebf-8840-7e1f9c8480d5",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Books/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff633f62-1181-4f81-adb0-24710c1c26e4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Other Considerations - Algorithms](18.03-mlpg-Other-Considerations-Algorithms.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Metrics and Error Analysis](18.05-mlpg-Other-Considerations-Metrics-and-Error-Analysis.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf0507-5b40-4c16-b91f-ecce3de4606b",
   "metadata": {},
   "source": [
    "# 18. Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc664d2-a245-4c0e-aad4-89dfe2915c68",
   "metadata": {},
   "source": [
    "## 18.4. Algorithm Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b976690-bd39-435d-9c26-ccebe0083920",
   "metadata": {},
   "source": [
    "### 18.4.1. Choosing between Random Forest vs SVM vs KNN\n",
    "![](figures/MLPG-OC-RFvsSVMvsKNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04429a79-6d35-4884-963c-886674cbe392",
   "metadata": {},
   "source": [
    "### 18.4.2. Is Random Forest a better model than a Decision Tree?\n",
    "* The answer is **YES** and **NO** \n",
    "* **YES** Random Forest is better if:\n",
    "  - The dataset is large\n",
    "  - Interpretability is not a major concern\n",
    "* **NO** Decision Trees are better if:\n",
    "  - The dataset is small\n",
    "  - Interpretability/understanding of the results is a major concern\n",
    "* Random forests are more accurate, more robust, and less prone to overfitting, but required higher training time compared to a single Decision Tree\n",
    "* In most practical applications, RFs are a common choice over DTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d384c3-41b6-45bb-8eab-eaa6c86d7bb3",
   "metadata": {},
   "source": [
    "### 18.4.3. Boosting and Bagging\n",
    "![](figures/MLPG-OC-BoostingAndBagging1.png)\n",
    "![](figures/MLPG-OC-BoostingAndBagging2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9752c-2f5a-4152-bc5f-ddef429c119b",
   "metadata": {},
   "source": [
    "There are some important differences between Boosting from Bagging are, in Boosting the:\n",
    "* Instances in the training set are assigned a weight based on difficulty\n",
    "  - The same training dataset is used to train each decision tree\n",
    "  - No sampling of the training dataset is performed, instead, each example in the training dataset (each row of data) is assigned a weight based on how easy or difficult the ensemble finds that example to predict\n",
    "  - This means that rows that are easy to predict using the ensemble have a small weight and rows that are difficult to predict correctly will have a much larger weight\n",
    "* Learning algorithms must pay attention to instance weights\n",
    "  - The base learning algorithm, e.g. the decision tree, must pay attention to the weightings of the training dataset, which means that boosting is specifically designed to use decision trees as the base learner, or other algorithms that support a weighting of rows when constructing the model\n",
    "  - The construction of the model must pay more attention to training examples proportional to their assigned weight which means that ensemble members are constructed in a biased way to make (or work hard to make) correct predictions on heavily weighted examples\n",
    "* Ensemble members are added sequentially\n",
    "  - The boosting ensemble is constructed slowly, which means the ensemble members are added sequentially, one, then another, and so on until the ensemble has the desired number of members\n",
    "  - The contribution of each model to the final prediction is a weighted sum of the performance of each model, e.g. a weighted average or weighted vote\n",
    "  - This incremental addition of ensemble members to correct errors on the training dataset sounds like it would eventually overfit the training dataset, however, In practice, boosting ensembles can overfit the training dataset, but often, the effect is subtle and overfitting is not a major problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b986d1-0059-4706-a6e8-11607ef5a8a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 18.4.4. SVM vs Logistic Regression\n",
    "Consider, **n** = _`number of features`_; **m** = _`number of observations`_\n",
    "\n",
    "* If **n** is large (relative to **m**): (e.g., n > m, n=10,000, m=10 to 1000)\n",
    "  - Use Logistic Regression or SVM without a kernel (“linear” kernel)\n",
    "* If **n** is small, **m** is intermediate: (e.g., n=1-1000, m=10 to 10,000)\n",
    "  - Use SVM with a kernel\n",
    "* If **n** is small, **m** is large: (e.g., n=1-1000, m=50,000+)\n",
    "  - Create/add more features, then\n",
    "  - Use Logistic Regression or SVM without a kernel (“linear” kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd8a00-0a68-4d84-8a11-567cfa43368e",
   "metadata": {},
   "source": [
    "### 18.4.5. Pros and Cons of NB, RF, GBDT, and NN classifiers\n",
    "![](figures/MLPG-OC-ProsConsNB-RF-GBDT-NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ea77e-8c67-40ac-9498-29334b7fd410",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Other Considerations - Algorithms](18.03-mlpg-Other-Considerations-Algorithms.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Other Considerations - Metrics and Error Analysis](18.05-mlpg-Other-Considerations-Metrics-and-Error-Analysis.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
