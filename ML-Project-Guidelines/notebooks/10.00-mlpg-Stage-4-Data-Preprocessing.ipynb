{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe2c5a3-04f0-49e4-abc0-6fca93cf74d3",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/MLPG-Book-Cover-Small.png\"><br>\n",
    "\n",
    "This notebook contains an excerpt from the **`Machine Learning Project Guidelines - For Beginners`** book written by *Balasubramanian Chandran*; the content is available [on GitHub](https://github.com/BalaChandranGH/Books/ML-Project-Guidelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec6c66-f315-48f6-83c7-603c2ae2dab0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<!--NAVIGATION-->\n",
    "\n",
    "<[ [Stage-3: Research](09.00-mlpg-Stage-3-Research.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Stage-5: Model Development](11.00-mlpg-Stage-5-Model-Development.ipynb) ]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605aae8-fc73-4e53-a800-8ab9b4884949",
   "metadata": {},
   "source": [
    "# 10. Stage-4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce2dc9-350e-47b2-8d0e-2491711a2771",
   "metadata": {},
   "source": [
    "* The success of the ML algorithms depends on the quality of the data and the data must be free from errors and discrepancies\n",
    "* It must adhere to a specific standard so that ML algorithms can accept them, but this does not happen in reality\n",
    "* In reality, the data is dirty, incomplete, noisy, and inconsistent\n",
    "* Incomplete data means it has missing values and lacks certain attributes\n",
    "* The data may be noisy as it contains errors and outliers and hence does not produce desired results\n",
    "* The data may be inconsistent as it contains discrepancies in data or duplicate data\n",
    "* ML practitioners take steps to transform the collected raw data and process them to meet the input requirements of model training and testing that are suitable for ML algorithms\n",
    "* It involves several steps for cleaning, transforming, normalizing, and standardizing data to remove all the inadequacies and irregularities in the data\n",
    "* These steps are collectively known as _**Data Preprocessing**_ (or) _**Data Wrangling**_ (or) _**Data Preparation**_ (or) _**Data Augmentation**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bfd28-aa5f-494c-ae0c-dd2e85cafefa",
   "metadata": {},
   "source": [
    "## 10.1. Data Preparation framework (for structured/tabular data)\n",
    "![](figures/MLPG-DataPrepFramework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e676f2b-b54a-4c70-bd3a-d851398803bd",
   "metadata": {},
   "source": [
    "## 10.2. Data Preparation tasks\n",
    "* **Data Cleaning**: Identifying and correcting mistakes or errors in the data\n",
    "* **Feature Selection**: Identifying those input variables that are most relevant to the task\n",
    "* **Feature Engineering**: Deriving new variables from available data\n",
    "* **Dimensionality Reduction**: Creating compact projections of the data\n",
    "* **Split datasets for train-test**: Separating datasets into input (X) and output (y) components\n",
    "* **Data Transforms**: Changing the scale or distribution of variables\n",
    "* **Handling Imbalanced Classes**: Imbalanced classes arise when one set of classes (majority class) dominates over another (minority class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195bb625-a216-41f9-9df2-119ee350add1",
   "metadata": {},
   "source": [
    "### 10.2.1. Data Cleaning\n",
    "* _``The process of identifying, correcting mistakes/ errors/ incomplete/ inconsistent/ noisy data, and preparing the dataset for analysis``_\n",
    "* The real-world dataset never comes clean; it comes in a wide variety of shapes and formats\n",
    "\n",
    "**Tidy data**\n",
    "* Tidy data provides a standard way to organize data values within a dataset\n",
    "* There are three principles of tidy data and they are:\n",
    "  - Columns represent separate variables\n",
    "  - Rows represent individual observations\n",
    "  - Observational units form tables\n",
    "* Tidy data makes it easier to fix common data problems, so, we need to transform the untidy dataset into tidy data\n",
    "\n",
    "**Signs of Untidy dataset**\n",
    "* _``Missing numerical data``_: Either they need to be deleted or replaced with a suitable test statistic\n",
    "* _``Unexpected data values``_: Solve the mismatched data types of a column and data values\n",
    "* _``Inconsistent column names``_: Address the column names that contain inconsistent caps and bad characters\n",
    "* _``Outliers``_: Remove or replace them with suitable test statistic as they can skew the results\n",
    "* _``Duplicate rows and columns``_: Drop them as they can cause bias in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff911c-e3ae-46b7-b68e-8e4a0880170e",
   "metadata": {},
   "source": [
    "#### 10.2.1.1. Basic data cleaning activities\n",
    "* Remove unwanted/duplicate columns/features/attributes\n",
    "* Remove unwanted/duplicate rows/samples/examples\n",
    "* Remove embedded characters that may cause data misalignment \n",
    "  - Eg., embedded tabs in a tab-separated data file, embedded new lines that may break records, etc\n",
    "* Identify the inconsistent values and bring them to a common standard of expression (eg., N.Y or NY into New York)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7889270-a1e9-4c81-8afe-9a9c975c9428",
   "metadata": {},
   "source": [
    "#### 10.2.1.2. Outliers detection\n",
    "* Outliers are extreme values that will skew the results if not addressed properly\n",
    "* There are many ways to identify outliers. Following are some of them:\n",
    "  - Box and Whisker plots\n",
    "  - Calculate Z-Score using the formula given below to identify the outliers (data points that fall outside a threshold)\n",
    "    $Z-Score = (x - Mean) / SD$\n",
    "* A couple of ways to handle outliers are:\n",
    "  - Drop them\n",
    "  - Replace with a suitable test statistic (such as mean/median/mode), examples are:\n",
    "    - _height_ column has a value of 0 which is invalid (outlier). It can be replaced with a mean value\n",
    "    ```\n",
    "      mean = df['height(cm)'].mean()\n",
    "      df['height(cm)'].replace(0.0, mean, inplace=True)\n",
    "    ```\n",
    "    - _weight_ column has a value of 190kg which may be mistakenly typed instead of 90kg\n",
    "    ```\n",
    "      df['weight(kg)'].replace(190.0, 90.0, inplace=True)\n",
    "    ```\n",
    "\n",
    "_**IMPORTANT NOTE**: Do outliers removals (discarding the samples or replacing the outlier values with 'mean' or 'median', 'mode' value) on training & test datasets separately_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63827acf-9c47-4fb3-afaa-b5081d712a7f",
   "metadata": {},
   "source": [
    "#### 10.2.1.3. Handling Missing Numerical values\n",
    "* Care must be taken while dealing with missing numerical values\n",
    "* Need to first identify the reason for the missing numerical values\n",
    "* There are several methods to handle missing values and each method has its advantages and disadvantages\n",
    "* The choice of the method is subjective and depends on the nature of the data and the missing values\n",
    "* Commands that help to detect the missing numerical values are:\n",
    "```\n",
    "isnull()\n",
    "isnull.sum()\n",
    "isna()\n",
    "notna()\n",
    "```\n",
    "* `Mark/Encode` missing numerical values with `NaN`:\n",
    "  - Missing values are encoded in different ways and they can appear as `NaN, NA, ?, 0, ‘xx’, -1, or \" \" (blank space)`\n",
    "  - Pandas always recognize missing values as `NaN`\n",
    "  - So, we must first convert all the `NA, ?, 0, xx, -1, or \" \" to NaN`\n",
    "  - If the missing values aren’t identified as `NaN`, then we have to first convert or replace such `non-NaN` entry with a `NaN`\n",
    "```\n",
    "df[df == '?'] = np.nan       # Convert '?' to 'NaN'\n",
    "```\n",
    "* `Drop` them using `dropna()` method\n",
    "  - This is the easiest method to handle missing values. In this method, we drop labels or columns from a dataset with missing values\n",
    "  - We can drop labels or rows from a dataset containing missing values as follows:\n",
    "```\n",
    "df.dropna(axis = 0)          # Drop rows with missing values\n",
    "df.dropna(axis = 1)          # Drop columns with missing values\n",
    "df.drop(‘col1’, axis = 1)    # Drop column ‘col1’\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea885af4-8c61-4c7e-b35c-21a499802610",
   "metadata": {},
   "source": [
    "**A note about axis parameter:**\n",
    "* Axis value may contain (0 or ‘index’) or (1 or ‘columns’). Its default value is 0\n",
    "* We set axis = 0 or ‘index’ to drop rows that contain missing values\n",
    "* We set axis = 1 or ‘columns’ to drop columns that contain missing values\n",
    "* But, this method has one disadvantage as it involves the risk of losing useful information along with the missing data\n",
    "* It is advised to use this method only when there are a few missing values in our dataset\n",
    "* It's better to develop an imputation strategy so that we can impute missing values with the mean or the median of the row or column containing the missing values\n",
    "* `Replace` with a suitable test statistic (such as mean/median/mode/forward-fill/back-fill), examples are:\n",
    "  - Fill the missing values with a test statistic like `mean`, `median`, or `mode` of the particular feature the missing value belongs to\n",
    "  - One can also specify a `forward-fill` or `back-fill` to propagate the next values backward or previous value forward\n",
    "  - We can fill missing values with a test statistic like `mean` as follows:\n",
    "    ```\n",
    "    mean = df['col_name'].mean()\n",
    "    df['col1'].fillna(value=mean, inplace=True )\n",
    "    ```\n",
    "  - We can also use replace() in place of fillna()\n",
    "    ```\n",
    "    df[‘col1’].replace(to_replace=NaN, value=mean, inplace=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12057067-6d3d-4b38-af2f-649be18c78ab",
   "metadata": {},
   "source": [
    "_**NOTE**: If we choose this method, then we should compute the mean value on the training set and use it to fill the missing values in the training set. Then we should save the mean value that we have computed. Later, we will replace missing values in the test set with the mean value to evaluate the system. This is to avoid DATA LEAKAGE._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3f979-86b5-495e-a1bd-c11e294bc49a",
   "metadata": {},
   "source": [
    "* `Impute`\n",
    "  - Scikit-Learn provides an Imputer class to deal with the missing values\n",
    "  - In this method, we replace the missing value with the mean value of the entire feature column. The sample code is given below\n",
    "    ```\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp = imp.fit(df.values)\n",
    "    imputed_data = imp.transform(df.values)\n",
    "    imputed_data\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79353cba-3bee-4113-907b-b5645176acfd",
   "metadata": {},
   "source": [
    "**Reshaping the data into tidy data format:**\n",
    "* If a dataframe is not in tidy format, we can convert it into the tidy data format using the `pd.melt()` function. For example:\n",
    "  ```\n",
    "  pd.melt(frame=df,id_vars=['fname','lname','age','sex','section',\n",
    "          'height(cm)','weight(kg)'], value_vars=['spend_A','spend_B', \n",
    "          spend_C'], var_name='expenditure', value_name='amount')`\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9f673-046e-4e48-92a4-738d2361277a",
   "metadata": {},
   "source": [
    "#### 10.2.1.4. Handling Missing Categorical values\n",
    "* Care must be taken while dealing with missing categorical values\n",
    "* Need to first identify the reason for the missing categorical values\n",
    "* In addition to the numerical values, the real-world datasets contain categorical data as well\n",
    "* ML algorithms require that some input data must be in numerical format; only then do the algorithms work successfully on them\n",
    "* The categorical data must be converted into numbers before they are fed into an algorithm. Scikit-learn provides useful classes to do the same\n",
    "* Some of the methods to handle missing values of categorical variables are:\n",
    "  - `Drop`, i.e., ignore the variables if it is not significant\n",
    "  - `Replace` it with the most frequent value (mode)\n",
    "  - `Treat` the missing data `as just another category`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b3e5b-0ba2-49b4-9134-c584e791ed6b",
   "metadata": {},
   "source": [
    "**Summary of Data Cleaning tasks:**\n",
    "![](figures/MLPG-DataCleaning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621aedb-9efe-44a2-962b-7c97513198dd",
   "metadata": {},
   "source": [
    "### 10.2.2. Feature Selection\n",
    "* Feature selection (FS) refers to techniques for selecting a subset of input features that are most relevant to the target variable that is being predicted\n",
    "* FS is primarily focused on removing non-informative or redundant predictors from the model\n",
    "* FS is about identifying those input variables that are most relevant to the task\n",
    "* FS removes columns with duplicate data/ empty/ unwanted data\n",
    "* FS techniques are generally grouped into those that use the target variable (`supervised`) and those that do not (`unsupervised`)\n",
    "* The difference has to do with whether features are selected based on the target variable or not. _**Unsupervised**_ feature selection techniques _**ignore the target variable**_, such as methods that remove redundant variables using correlation. _**Supervised**_ feature selection techniques _**use the target variable**_, such as methods that remove irrelevant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56e659-517f-498a-8664-6e9520208176",
   "metadata": {},
   "source": [
    "#### 10.2.2.1. The Goals of Feature Selection Techniques\n",
    "* To reduce the computational cost of modeling\n",
    "* To improve the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24a65a-f85d-4bd0-92bf-60a2086b1bc3",
   "metadata": {},
   "source": [
    "#### 10.2.2.2. Overview of Feature Selection Techniques\n",
    "* **Feature Selection**: Select a subset of input features from the dataset\n",
    "  - **Unsupervised**: Do not use the target variable (e.g. remove redundant variables)\n",
    "  - **Supervised**: Use the target variable (e.g. remove irrelevant variables)\n",
    "    - **Wrapper**: Search for well-performing subsets of features (i.e., explicitly choose features that result in the best performing model)\n",
    "    - **Filter**: Select subsets of features based on their relationship with the target (i.e., score each input feature and allow a subset to be selected)\n",
    "    - **Intrinsic**: Algorithms that perform automatic feature selection during training. \n",
    "  - Some models are naturally resistant to non-informative predictors. Tree- and rule-based models, MARS and the Lasso, for example, intrinsically conduct FS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0088010-aa8f-4962-b0a7-793e3b8361fa",
   "metadata": {},
   "source": [
    "* **Dimensionality Reduction:** \n",
    "  - Project input data into a lower-dimensional feature space\n",
    "  - Dimensionality reduction (eg., PCA) is an alternative to FS rather than a type of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6a38a-0afa-4c93-85be-bbf92c1ead91",
   "metadata": {},
   "source": [
    "![](figures/MLPG-FeatureSelectionTechs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d6c29-fe6a-44ee-a913-389ebd048649",
   "metadata": {},
   "source": [
    "#### 10.2.2.3. Feature Selection Methods (Filter-based)\n",
    "**Scikit-learn** implementations:\n",
    "* Pearson’s Correlation Coefficient: [f_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "* ANOVA: [f_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)\n",
    "* Chi-Squared: [chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "* Mutual Information: [mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) & [mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "* Select the top k variables: [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "* Select the top percentile variables: [SelectPercentile](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)\n",
    "\n",
    "**SciPy** implementations:\n",
    "* Kendall’s tau: [kendalltau](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)\n",
    "* Spearman’s rank correlation: [spearmanr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b9a8e-d61e-404d-9d0b-ba7a96eda074",
   "metadata": {},
   "source": [
    "_**NOTE**: Just like there is no best set of input variables or best ML algorithm, there is no best feature selection method. One must discover what works best for a specific problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191bd40-f318-49fe-bb44-6f53e17f9b54",
   "metadata": {},
   "source": [
    "![](figures/MLPG-FeatureSelectionMethods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9da88-f223-496e-a8f9-5ddeea31c001",
   "metadata": {},
   "source": [
    "#### 10.2.2.4. Feature Importance\n",
    "**Feature importance (FI)** refers to techniques that assign a score to input features based on how useful they are at predicting a target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bdaeb-becf-403d-a1b7-79c21c34ef72",
   "metadata": {},
   "source": [
    "#### 10.2.2.4.1. The Uses of Feature Importance\n",
    "* Better understanding the data – FI scores can provide insight into the dataset\n",
    "* Better understanding a model – FI scores can provide insight into the model\n",
    "* Reducing the number of input features – FI scores can be used to improve a predictive model\n",
    "* Many ways to calculate FI scores and many models can be used for this purpose. Commonly used are,\n",
    "  - Feature importance from model coefficients\n",
    "  - Feature importance from decision trees\n",
    "  - Feature importance from permutation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cb2e4-3a40-4467-80d0-10b46d8b44c6",
   "metadata": {},
   "source": [
    "#### 10.2.2.4.2. Coefficients as Feature Importance\n",
    "* In **Sklearn.linear_model** (LinearRegression, LogisticRegression, Ridge, ElasticNet), the `model.coeff_` property contains the coefficients found for each input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf4a22-cbb3-4acd-8373-558df0823afb",
   "metadata": {},
   "source": [
    "#### 10.2.2.4.3. Decision Tree Feature Importance\n",
    "* In **Sklearn.tree** (CART FI [DecisionTreeRegressor, DecisionTreeClassifier], Random Forest FI [RandomForestRegressor, RandomForestClassifier]), the `model.feature_importances_` property contains the coefficients found for each input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34ca6d-28be-4c14-9261-4da698bef657",
   "metadata": {},
   "source": [
    "#### 10.2.2.4.4. XGBoost Feature Importance\n",
    "* In **xgboost** (XGBRegressor, XGBClassifier), the `model.feature_importances_` property contains the coefficients found for each input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf25c9-0932-4db7-a50a-57c8a9b33415",
   "metadata": {},
   "source": [
    "#### 10.2.2.4.5. Permutation Feature Importance\n",
    "* **Permutation Feature Importance** is a technique for calculating relative importance scores that is independent of the model used\n",
    "* This approach can be used for regression or classification and requires that a performance metric be chosen as the basis of the importance score, such as the mean squared error for regression and accuracy for classification\n",
    "* Permutation feature selection can be used via the [permutation_importance() function](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) that takes a fit model, a dataset (train or test dataset is fine), and a scoring function (available in `scikit-learn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db1555-e4f3-40f8-8480-b819e832459f",
   "metadata": {},
   "source": [
    "#### 10.2.2.5. Mutual Information (MI)\n",
    "* MI is a lot like correlation in that it measures a relationship between two quantities\n",
    "* MI describes relationships in terms of uncertainty\n",
    "* The MI between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other\n",
    "* _`The benefit of MI is that it can detect any kind of relationship, while correlation only detects linear relationships`_\n",
    "* MI is a great general-purpose metric and especially useful at the start of feature development when we might not know what model we’d like to use yet\n",
    "* `Advantages:`\n",
    "  - Easy to use and interpret\n",
    "  - Computationally efficient\n",
    "  - Theoretically well-founded\n",
    "  - Resistant to overfitting\n",
    "  - Able to detect any kind of relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdceda-0c99-440a-8841-cfc2b5c6bc4b",
   "metadata": {},
   "source": [
    "_**NOTE**: The uncertainty is measured using a quantity from information theory known as \"entropy\". The entropy of a variable means roughly: \"how many yes-or-no questions you would need to describe an occurrence of that variable, on average.\" The more questions you have to ask, the more uncertain you must be about the variable. MI is how many questions you expect the feature to answer about the target._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01998b-9dd4-48fe-bb07-19f6c5739bfe",
   "metadata": {},
   "source": [
    "#### 10.2.2.6. Interpreting MI scores\n",
    "* MI score range -> _**zero to inf**_ (0.0 to ꝏ).\n",
    "* When MI is zero, the quantities are independent: neither can tell you anything about the other\n",
    "* Conversely, in theory, there's no upper bound to what MI can be\n",
    "* In practice, though values above 2.0 or so are uncommon\n",
    "* MI is a logarithmic quantity, so it increases very slowly\n",
    "* Things to remember when applying mutual information:\n",
    "  - MI can help understand the _`relative potential`_ of a feature as a predictor of the target, considered by itself\n",
    "  - It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. _`MI can't detect interactions between features.`_ It is a **univariate** metric.\n",
    "  - The actual usefulness of a feature _`depends on the model we use it with`_. A feature is only useful to the extent that its relationship with the target is one the model can learn. Just because a feature has a high MI score doesn't mean the model will be able to do anything with that information. We may need to transform the feature first to expose the association\n",
    "* Scikit-learn has two MI metrics in its `feature_selection` module: \n",
    "  - One for real-valued targets (`mutual_info_regression`) \n",
    "  - One for categorical targets (`mutual_info_classif`)\n",
    "* Example source code to rank the features with MI and investigate the results by data visualization\n",
    "[1985 Automobiles (auto.csv) dataset is available in Kaggle]:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b07dd1f1-3207-4622-8958-5c529f2ec3c4",
   "metadata": {},
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "df = pd.read_csv(\"../autos.csv\")\n",
    "X  = df.copy()\n",
    "y  = X.pop(\"price\")\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI)\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores\n",
    "\n",
    "def plot_utility_scores(scores):\n",
    "    y = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(y))\n",
    "    ticks = list(y.index)\n",
    "    plt.barh(width, y)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_utility_scores(mi_scores)\n",
    "\n",
    "sns.relplot(x=\"curb_weight\", y=\"price\", data=df);\n",
    "\n",
    "sns.lmplot(x=\"horsepower\", y=\"price\", hue=\"fuel_type\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520e93e-9df8-4126-a591-523d1f112d49",
   "metadata": {},
   "source": [
    "### 10.2.3. Feature Engineering\n",
    "* _**“Better features make better data and better data make better models”.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedd8e4-859d-4576-a3b0-54a61e54f559",
   "metadata": {},
   "source": [
    "#### 10.2.3.1. The Goal and a Guiding principle\n",
    "* The goal of Feature Engineering is to make data better suited to the problem to be solved\n",
    "* Establish a baseline (score) by training the model on the un-augmented dataset. This will help us determine whether the new features are useful/worth keeping or discard them and try something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ca37b-f584-4e89-b738-af8ac73fe95d",
   "metadata": {},
   "source": [
    "#### 10.2.3.2. Creating features\n",
    "* Tips on discovering new features:\n",
    "  - Understand the existing features in the dataset by referring to the “data description document”\n",
    "  - Research the problem domain to acquire **domain knowledge**\n",
    "  - Study previous work. Solution write-ups are a great resource\n",
    "  - Use data visualization\n",
    "* Complex strings that can usefully be broken into simpler pieces. Some common examples are:\n",
    "  - ID numbers: `'123-45-6789'`\n",
    "  - Phone numbers: `'(999) 555-0123'`\n",
    "  - Street addresses: `'8241 Kaggle Ln., Goose City, NV'`\n",
    "  - Internet addresses: `‘http://www.kaggle.com’`\n",
    "  - Product codes: `'0 36000 29145 2'`\n",
    "  - Dates and times: `'Mon Sep 30 07:06:05 2013'`\n",
    "  - Split columns (Area code from Phone numbers or Year from Dates and times, etc.)\n",
    "* Combine/aggregate existing features to create a new one (e.g., Total or Count or Mean or Ratio)\n",
    "* Reorder columns, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ae1cd-b80f-43d6-89a5-52e3c8b30991",
   "metadata": {},
   "source": [
    "#### 10.2.3.3. Tips on creating new features\n",
    "Keep in mind the model's strengths and weaknesses when creating features. Here are some guidelines: \n",
    "* Linear models learn sums and differences naturally, but can't learn anything more complex\n",
    "* Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains\n",
    "* Linear models and NNs generally do better with normalized features. NNs especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so\n",
    "* Tree models can learn to approximate almost any combination of features, but when a combination is especially important, they can still benefit from having it explicitly created, especially when data is limited\n",
    "* Counts are especially helpful for tree models since these models don't have a natural way of aggregating information across many features at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b2827-3552-4c4d-887d-45fe8d5d4978",
   "metadata": {},
   "source": [
    "#### 10.2.3.4. Clustering with K-Means\n",
    "* We can use clustering algorithms (such as K-Means) in feature engineering. For example, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patterns. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity\n",
    "* Cluster Labels as a Feature\n",
    "  - Applied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. On multiple features, it's like \"multi-dimensional binning\" (sometimes called VECTOR QUANTIZATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4e952-c2e2-4a30-a7fa-770faf501eaf",
   "metadata": {},
   "source": [
    "#### 10.2.3.5. Principal Component Analysis (PCA)\n",
    "* Just like clustering is a partitioning of the dataset based on _`proximity`_, we could think of PCA as a partitioning of the _`variation`_ in the data\n",
    "* PCA is a great tool to help discover important relationships in the data and can also be used to create more informative features\n",
    "* _**NOTE**: PCA is typically applied to standardized data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\"._\n",
    "* There are two ways you could use PCA for feature engineering:\n",
    "  - The **first way** is to use it as a **descriptive technique**. Since the components tell us about the variation, we could compute the MI scores for the components and see what kind of variation is most predictive of our target. That could give us ideas for kinds of features to create: \n",
    "    - A product of `'Height'` and `'Diameter'` if `'Size'` is important, say, or a ratio of `'Height'` and `'Diameter'` if `'Shape'` is important. You could even try clustering on one or more of the high-scoring components\n",
    "  - The **second way** is to use the **components themselves as features**. Because the components expose the variational structure of the data directly, they can often be more informative than the original features\n",
    "  - `Use cases for PCA:`\n",
    "    - **Dimensionality reduction**: When the features are highly redundant (_`multicollinear`_, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which we can then drop since they will contain little or no information\n",
    "    - **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task\n",
    "    - **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio\n",
    "    - **Decorrelation**: Some ML algorithms struggle with highly correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for the algorithm to work with\n",
    "* `PCA Best Practices:`\n",
    "  - PCA only works with numeric features, like continuous quantities or counts\n",
    "  - PCA is sensitive to scale. It's good practice to standardize the data before applying PCA unless we know we have a good reason not to\n",
    "  - Consider removing or constraining outliers, since they can have an undue influence on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122bfd32-9ce3-4e9b-9938-9d7dca48f221",
   "metadata": {},
   "source": [
    "#### 10.2.3.6. Target Encoding\n",
    "All the techniques discussed above so far are for numerical features. The technique used to encode categorical features is called “target encoding”\n",
    "* A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target\n",
    "* Target encoding is sometimes called _`mean encoding`_ or _`likelihood encoding`_ or _`impact encoding`_ or _`leave-one-out encoding`_ or _`binary encoding`_ (if applied to a binary target)\n",
    "* Use cases for Target Encoding:\n",
    "  - **High-cardinality features**: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target\n",
    "  - **Domain-motivated features**: From prior experience, we might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5963557-bae5-4af9-8891-90fed17f8475",
   "metadata": {},
   "source": [
    "### 10.2.4. Dimensionality Reduction\n",
    "* Creating compact projections of the data\n",
    "* Dimensionality reduction can be considered as part of Feature Selection as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790c8b-1e37-4cf5-b408-8a263a81b027",
   "metadata": {},
   "source": [
    "### 10.2.5. Split datasets for train-test\n",
    "* Separate datasets into Input (X) and output (y) components, if needed\n",
    "* Split the preprocessed datasets into train and test datasets, for model development, training, and evaluations\n",
    "  - _`train-test-split`_:\n",
    "    - A procedure to evaluate the performance of models on a large dataset (less computational cost)\n",
    "    - The dataset split can be done during the Preprocessing stage and the train & test datasets should be used at later stages\n",
    "    - _`Use train-test-split()`_ function from sklearn.model_selection\n",
    "    - `k` is internally set to 2 for a single split (1 train and 1 test dataset)\n",
    "  - _`LOOCV (Leave-One-Out-Cross-Validation)`_:\n",
    "    - This is another extreme to `train-test-split` where `k` is set to the total number of observations (`k=n`) such that each observation is given a chance to be held out of the dataset\n",
    "    - _`Use LeaveOneOut()`_ class from _sklearn.model_selection_\n",
    "* Remove outliers (replace with a suitable test statistic `mean/median/mode`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0e861-fbd8-4b1c-9a67-d935984e5dbc",
   "metadata": {},
   "source": [
    "### 10.2.6. Data Transforms\n",
    "* Changing the scale or distribution of variables.\n",
    "* Data transformation can be considered as part of Feature engineering as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cbf05-335e-4ae3-b9c4-4cc297ea54a4",
   "metadata": {},
   "source": [
    "#### **10.2.6.1. Numerical type**\n",
    "#### **Change scale**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98eba4-5104-44b9-8343-84188828a531",
   "metadata": {},
   "source": [
    "#### 10.2.6.1.1. Normalization a.k.a. Feature Scaling\n",
    "* Rescales numerical values to a specific range of 0-1 to reduce skews\n",
    "* Feature Scaling is a process used to normalize the range of independent variables so that they can be mapped onto the same scale\n",
    "* In stochastic gradient descent, feature scaling can improve the convergence speed of the algorithm\n",
    "* In SVMs, it can reduce the time to find support vectors\n",
    "* Exceptions:\n",
    "  - `Decision trees` and `random forests` are scale-invariant algorithms where we don’t need to worry about feature scaling\n",
    "  - Similarly, `Naive Bayes` and `Linear Discriminant Analysis` are not affected by feature scaling\n",
    "  - In Short, `any Algorithm which is not distance-based is not affected by feature scaling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ad1af-1094-448d-bf53-876e7cf5b5c2",
   "metadata": {},
   "source": [
    "* Some of the common methods are:\n",
    "  - _`Log`_ transform\n",
    "  - _`MinMaxScaler`_ transform\n",
    "    - This technique of rescaling is also called min-max scaling or min-max normalization\n",
    "    - Normalization refers to the rescaling of the features to a range of [0, 1], which is a special case of min-max scaling\n",
    "    - We do this by subtracting the minimum value (xmin) and dividing it by the maximum value (xmax) minus the minimum value (xmin)\n",
    "    - Mathematically, the new value x(i)norm of sample x(i) can be calculated as follows:\n",
    "      ```\n",
    "      x(i)norm  =  (xi -  xmin)/(xmax -  xmin)\n",
    "      ```\n",
    "    - Here, `x(i)` is a particular sample value. xmax and xmin are the maximum and minimum feature values in a column. Scikit-Learn provides a transformer called _`MinMaxScaler`_ for this task. It has a feature range parameter to adjust the range of values. This estimator fits and transforms each feature variable individually such that it is in the given range (between zero and one) on the training set\n",
    "    - As with all the other transformers, we fit this transformer to the training data only, not to the full data set (including the test set to avoid data leakage). Only then we can use them to transform the training set and the test set and new data. The syntax for implementing the min-max scaling procedure in Scikit-Learn is given as follows:\n",
    "      ```\n",
    "      from sklearn.preprocessing import MinMaxScaler\n",
    "      ms = MinMaxScaler()\n",
    "      X_train_ms = ms.fit_transform(X_train)\n",
    "      X_test_ms  = ms.transform(X_test)\n",
    "      ```\n",
    "  - _`MaxAbsScaler`_ transform\n",
    "    - In this feature rescaling task, we rescale each feature by its maximum absolute value\n",
    "    - So, the maximum absolute value of each feature in the training set will be 1.0\n",
    "    - It does not affect the data and hence there is no effect on sparsity\n",
    "    - Scikit-Learn provides _`MaxAbsScaler`_ transformer for this task\n",
    "    - The syntax for implementing max-abs scaling procedure in Scikit-Learn is given as follows:\n",
    "      ```\n",
    "      from sklearn.preprocessing import MaxAbsScaler\n",
    "      mabs = MaxAbsScaler()\n",
    "      X_train_mabs = mabs.fit_transform(X_train)\n",
    "      X_test_mabs  = mabs.transform(X_test)\n",
    "      ```\n",
    "  - _`Normalizer`_ transform\n",
    "    - In this feature scaling task, we rescale each observation to a length of 1 (a unit norm). \n",
    "    - Scikit-Learn provides the _`Normalizer`_ class for this task\n",
    "    - In this task, we scale the components of a feature vector such that the complete vector has a length of one\n",
    "    - This usually means dividing each component by the Euclidean length (magnitude) of the vector\n",
    "    - Mathematically, normalization can be expressed by the following equation:\n",
    "      ```\n",
    "      x(i)norm =   x(i) / | x(i)|\n",
    "      ```\n",
    "      where `x(i)` is a particular sample value, `x(i)norm` is its normalized value, and `| x(i)|` is the corresponding Euclidean length of the vector. The syntax for normalization is quite similar to standardization given as follows:\n",
    "      ```\n",
    "      from sklearn.preprocessing import Normalizer\n",
    "      nm = Normalizer()\n",
    "      X_train_nm = nm.fit_transform(X_train)\n",
    "      X_test_nm  = nm.transform(X_test)\n",
    "      ```\n",
    "  - _`Binarizer`_ transform\n",
    "    - In this feature scaling procedure, we binarize the data (set feature values equal to 0 or 1) according to a threshold\n",
    "    - Using a binary threshold, we transform our data by marking the values above it to 1 and those equal to or below it to 0\n",
    "    - Scikit-Learn provides Binarizer class for this purpose. The syntax for binarizing the data follow the same rules as above and is given below:\n",
    "      ```\n",
    "      from sklearn.preprocessing import Binarizer\n",
    "      binr = Binarizer()\n",
    "      X_train_binr = binr.fit_transform(X_train)\n",
    "      X_test_binr  = binr.transform(X_test)\n",
    "      ```\n",
    "  - _`Decimal`_ scaling: Scale the data by moving the decimal point of values of the attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5b437-a136-4817-b9db-451ddc9f375b",
   "metadata": {},
   "source": [
    "**Applications of Feature scaling:**\n",
    "* Generally, real-world datasets contain features that differ in magnitudes, units, and ranges\n",
    "* We should perform Normalization when the scale of a feature is irrelevant or misleading\n",
    "* The algorithms which depend on Euclidean distance measures are sensitive to magnitudes\n",
    "* In this case, feature scaling helps to weigh all the features equally\n",
    "* Suppose a feature in the dataset is relatively big in magnitude as compared to other features, then in algorithms where Euclidean distance is measured, this big scaled feature becomes dominating and needs to be normalized\n",
    "* Applications of Feature Scaling:\n",
    "  - _`K-Means`_: K-Means algorithm is based on the Euclidean distance measure, so, feature scaling matters\n",
    "  - _`K-Nearest-Neighbours`_: Require feature scaling\n",
    "  - _`Principal Component Analysis (PCA)`_: In the PCA algorithm, we try to get the feature with maximum variance. Here feature scaling is required\n",
    "  - _`Gradient Descent`_: In gradient descent algorithm, calculation speed increases as theta calculation becomes faster after feature scaling\n",
    "  - _`Naive Bayes, Linear Discriminant Analysis, and Tree-Based models (Decision Trees and Random Forests) are not affected by feature scaling because they are not distance-based.`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11022b62-b2ec-4d4f-a784-de1a29bcf231",
   "metadata": {},
   "source": [
    "#### 10.2.6.1.2. Standardization (_`StandardScaler`_)\n",
    "* Standardize numerical data using the scale and center options (i.e., Mean of 0 and SD of 1))\n",
    "* It can be more useful for many ML algorithms, especially for optimization algorithms such as gradient descent\n",
    "* In standardization, first, we determine the distribution mean and standard deviation for each feature. \n",
    "* Next, we subtract the mean from each feature, then we divide the values of each feature by its standard deviation\n",
    "* So, in standardization, we center the feature columns at mean 0 with a standard deviation of 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights\n",
    "* Mathematically, standardization can be expressed by the following equation:\n",
    "  ```\n",
    "  x(i)std =  ( x(i)- μx)/(σx )\n",
    "  ```\n",
    "* Here, `x(i)` is a particular sample value, `x(i)std` is its standard deviation, `μx` is the sample mean of a particular feature column and `σx` is the corresponding standard deviation\n",
    "* Min-max scaling scales the data to a limited range of values. Unlike min-max scaling, standardization does not bind values to a specific range\n",
    "* So, standardization is much less affected by outliers\n",
    "* Standardization maintains useful information about outliers and is much less affected by them\n",
    "* It makes the algorithm less sensitive to outliers in contrast to min-max scaling\n",
    "* Scikit-Learn provides a transformer called _`StandardScaler`_ for standardization\n",
    "* The syntax to implement standardization is quite similar to min-max scaling given as follows:\n",
    "  ```\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  ss = StandardScaler()\n",
    "  X_train_ss = ss.fit_transform(X_train)\n",
    "  X_test_ss  = ss.transform(X_test)\n",
    "  ```\n",
    "* Again, we should fit the _`StandardScaler`_ class only once on the training data set and use those parameters to transform the test set or new data set to avoid data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b9e72-e2d3-4ee8-a335-f871c06a83b5",
   "metadata": {},
   "source": [
    "#### 10.2.6.1.3. Robust (_`RobustScaler`_)\n",
    "* _`StandardScaler`_ can often give misleading results when the data contain outliers\n",
    "* Outliers can often influence the sample mean and variance and hence give misleading results\n",
    "* In such cases, it is better to use a scalar that is robust against outliers\n",
    "* Scikit-Learn provides a transformer called _`RobustScaler`_ for this purpose\n",
    "* The _`RobustScaler`_ is very similar to _`MinMaxScaler`_. The difference lies in the parameters used for scaling. While _`MinMaxScaler`_ uses the minimum and maximum values for rescaling, _`RobustScaler`_ uses the interquartile (IQR) range for the same\n",
    "* Mathematically, the new value x(i)norm of sample x(i) can be calculated as follows:\n",
    "  ```\n",
    "  x(i)  =  (xi-  Q1(x) )/(Q3(x) - Q1(x))\n",
    "  ```\n",
    "* Here, `x(i)` is the scaled value, `xi` is a particular sample value, and `Q1(x)` and `Q3(x)` are the 1st quartile (25th quantile) and 3rd quartile (75th quantile) respectively. So, `Q3(x) - Q1(x)` is the difference between the 3rd quartile (75th quantile) and 1st quartile (25th quantile) respectively. It is called `IQR (Interquartile Range)`\n",
    "* The syntax for implementing scaling using _`RobustScaler`_ in Scikit-Learn is given as follows:\n",
    "  ```\n",
    "  from sklearn.preprocessing import RobustScaler\n",
    "  rb = RobustScaler()\n",
    "  X_train_rb = rb.fit_transform(X_train)\n",
    "  X_test_rb  = rb.transform(X_test)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154e90f-5d5d-4ba6-9a67-46760706f534",
   "metadata": {},
   "source": [
    "#### **Change distribution**\n",
    "\n",
    "#### 10.2.6.1.4. Power (_`PowerTransformer`_)\n",
    "* A power transform will make the probability distribution of a variable more Gaussian distribution\n",
    "* This power transform is available in the scikit-learn Python machine learning library via the _`PowerTransformer`_ class with the methods:\n",
    "  - _`Box-Cox`_ transform: Automatic power transform\n",
    "  - _`Yeo-Johnson`_ transform: Automatic power transform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c577736-1ae5-4a88-be12-a3561427d17b",
   "metadata": {},
   "source": [
    "#### 10.2.6.1.5. Quantile (_`QuantileTransformer`_)\n",
    "* Numerical input variables may have a highly skewed or non-standard distribution\n",
    "* The quantile transform provides an automatic way to transform a numeric input variable to have a different data distribution, which in turn, can be used as input to a predictive model\n",
    "* A quantile transform will map a variable’s probability distribution to another probability distribution\n",
    "* The quantile function ranks or smooths out the relationship between observations and can be mapped onto other distributions, such as the uniform or normal distribution\n",
    "* This quantile transform is available in the scikit-learn Python machine learning library via the _`QuantileTransformer`_ class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b16d1e-5670-4414-97f3-8d1fb6876301",
   "metadata": {},
   "source": [
    "#### 10.2.6.1.6. Discretize (_`KBinsDiscretizer`_)\n",
    "* This is also called _`BINNING`_ i.e., a grouping of values into 'bins' (e.g. High, Medium, Low)\n",
    "* The discretization transform provides an automatic way to change a numeric input variable to have a different data distribution, which in turn can be used as input to a predictive model\n",
    "* Different methods for grouping the values into k discrete bins can be used; common techniques include:\n",
    "  - _`Uniform`_: Each bin has the same width in the span of possible values for the variable\n",
    "  - _`Quantile`_: Each bin has the same number of values, split based on percentiles\n",
    "  - _`Clustered`_: Clusters are identified and examples are assigned to each group\n",
    "* The discretization transform is available in the scikit-learn Python machine learning library via the _`KBinsDiscretizer`_ class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2277c7-80ed-48d3-9b75-fda9646af997",
   "metadata": {},
   "source": [
    "#### **Engineer**\n",
    "\n",
    "#### 10.2.6.1.7. Polynomial (_`PolynomialFeatures`_)\n",
    "* Polynomial features are those features created by raising existing features to an exponent\n",
    "* The polynomial features transform is available in the scikit-learn Python machine learning library via the _`PolynomialFeatures`_ class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c2f2a-1ae6-404a-bcd4-ebc02f626471",
   "metadata": {},
   "source": [
    "#### **10.2.6.2. Categorical type**\n",
    "\n",
    "#### **Nominal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c1746-7334-4b8e-af8d-81f00af58f2e",
   "metadata": {},
   "source": [
    "#### 10.2.6.2.1. One-Hot encode (_`OneHotEncoder`_)\n",
    "* For categorical variables that do not have a natural rank-ordering, i.e., no relationships\n",
    "  ```\n",
    "  Eg., 'red' is (1,0,0), 'green' is (0,1,0), and 'blue' is (0,0,1)\n",
    "  ```\n",
    "* _`LabelEncoder`_ treats class labels as categorical data with no order associated with it\n",
    "* The problem arises when we apply the same approach to transform the nominal variable with _`LabelEncoder`_\n",
    "* Refer example in section 4.2.3.2.4. LabelEncode, the values are encoded as 0, 1, 2 for 'high', 'low', 'medium' respectively. This is OK for ordinal variables but not for nominal variables\n",
    "* Although there is no order involved, a learning algorithm will assume that _`high < low < medium`_. This is a wrong assumption and it will not produce desired results\n",
    "* To fix this issue, a common solution is to use a technique called _`one-hot-encoding`_\n",
    "* In this technique, we create a new dummy feature for each unique value in the nominal feature column. The value of the dummy feature is equal to one when the unique value is present and zero otherwise. Similarly, for another unique value, the value of the dummy feature is equal to one when the unique value is present and zero otherwise. This is called one-hot encoding because only one dummy feature will be equal to one (hot), while the others will be zero (cold)\n",
    "* Scikit-Learn provides an OneHotEncoder transformer to convert integer categorical values into one-hot vectors. For example:\n",
    "from sklearn.preprocessing import _`OneHotEncoder`_\n",
    "  ```\n",
    "  x   = ['high', 'medium', 'low', 'low', 'high']\n",
    "  df  = pd.DataFrame(x, columns=['x'])\n",
    "  col = df['x'].values.reshape(-1,1)\n",
    "  ohe = OneHotEncoder(sparse=False)\n",
    "  df  = ohe.fit_transform(col)\n",
    "  <<Output>> array([[1., 0., 0.],\n",
    "                    [0., 0., 1.],\n",
    "                    [0., 1., 0.],\n",
    "                    [0., 1., 0.],\n",
    "                    [1., 0., 0.]])\n",
    "  ```\n",
    "* By default, the output is a SciPy sparse matrix, instead of a NumPy array. This way of output is very useful when we have categorical attributes with thousands of categories. If there are a lot of zeros, a sparse matrix only stores the location of the non-zero elements. So, sparse matrices are a more efficient way of storing large datasets. It is supported by many Scikit-Learn functions\n",
    "* To convert the dense NumPy array, we should call the toarray() method. To omit the toarray() step, we could alternatively initialize the encoder as:\n",
    "  ```\n",
    "  OneHotEncoder( … , sparse=False)   # Returns a regular NumPy array\n",
    "  ```\n",
    "* Another more convenient way is to create those dummy features via one-hot encoding is to use the _`pandas.get_dummies()`_ method. The _`get_dummies()`_ method will only convert string columns and leave all other columns unchanged in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dc5f7-8f23-4c8f-8fc8-60e432aa79b6",
   "metadata": {},
   "source": [
    "#### 10.2.6.2.2. Dummy encode\n",
    "* Dummy encoding: Avoid redundancy in One-Hot encoding\n",
    "  ```\n",
    "  Eg., 'red' is (1,0), 'green' is (0,1), and 'blue is (0,0)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd97646-629a-435a-ad36-2f2fc4bbb1cf",
   "metadata": {},
   "source": [
    "#### 10.2.6.2.3. Label binarize (_`LabelBinarizer`_)\n",
    "* We can accomplish two tasks (encoding multi-class labels to integer categories, then from integer categories to one-hot vectors or binary labels) in one shot using the Scikit-Learn’s _`LabelBinarizer`_ class, in other words, it combines `Label encode + One-hot encode`. For example:\n",
    "```\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "x   = ['high', 'medium', 'low', 'low', 'high']\n",
    "#x = [1.3, 3.1, 2.2, 6.4]\n",
    "df  = pd.DataFrame(x, columns=['x'])\n",
    "col = df['x'].values\n",
    "#col = df['x'].values.astype('int64')\n",
    "lb  = LabelBinarizer()\n",
    "df  = lb.fit_transform(col)\n",
    "<<Output>> array([[1, 0, 0],\n",
    "                  [0, 0, 1],\n",
    "                  [0, 1, 0],\n",
    "                  [0, 1, 0],\n",
    "                  [1, 0, 0]])\n",
    "```\n",
    "* This returns a dense NumPy array by default. We can get a sparse matrix by passing _`sparse_output=True`_ to the _`LabelBinarizer`_ constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf427430-d0cd-47f1-8795-92943cc3e663",
   "metadata": {},
   "source": [
    "#### **Ordinal**\n",
    "\n",
    "#### 10.2.6.2.4. Label encode (_`LabelEncoder`_)\n",
    "* The ML algorithms require that class labels are encoded as integers and most estimators for classification convert class labels to integers internally\n",
    "* Scikit-Learn provides a transformer for this task called _`LabelEncoder`_. For example:\n",
    "  ```\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "  x   = ['high', 'medium', 'low', 'low', 'high']\n",
    "  df  = pd.DataFrame(x, columns=['x'])\n",
    "  col = df['x'].values\n",
    "  le  = LabelEncoder()\n",
    "  df['x'] = le.fit_transform(col)\n",
    "  <<Output>> 0 2 1 1 0\n",
    "  ```\n",
    "* We can use the _`inverse_transform`_ method to transform the integer class labels back into their original string representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bcb61-a7d8-4029-ad91-cba8e46ea957",
   "metadata": {},
   "source": [
    "#### 10.2.6.2.5. Ordinal encode (_`OrdinalEncoder`_)\n",
    "* For categorical variables that do not have a natural rank/ ordering, i.e., each unique category value is assigned an integer value\n",
    "  ```\n",
    "  Eg., 'red' is 1, 'green' is 2, and 'blue' is 3\n",
    "  ```\n",
    "* For example:\n",
    "  ```\n",
    "  from sklearn.preprocessing import OrdinalEncoder\n",
    "  x1   = ['high1', 'medium1', 'low1', 'low1', 'high1']\n",
    "  x2   = ['high2', 'medium2', 'low2', 'low2', 'high2']\n",
    "  x3   = ['high3', 'medium3', 'low3', 'low3', 'high3']\n",
    "  data = {'x1':x1, 'x2':x2, 'x3':x3}\n",
    "  df   = pd.DataFrame(data)\n",
    "  col  = df[['x1', 'x2', 'x3']]\n",
    "  oe   = OrdinalEncoder()\n",
    "  df   = oe.fit_transform(col)\n",
    "  <<Output>> array([[0., 0., 0.],\n",
    "                    [2., 2., 2.],\n",
    "                    [1., 1., 1.],\n",
    "                    [1., 1., 1.],\n",
    "                    [0., 0., 0.]])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525bfa6-9775-49d3-bc71-f6a1b9cdb7f0",
   "metadata": {},
   "source": [
    "**Differences between LabelEncoder and OrdinalEncoder**\n",
    "```\n",
    "LabelEncoder                               OrdinalEncoder\n",
    "--------------------------------------     -----------------------------------------------\n",
    "- Deals with 1D data, i.e., n_samples      - Deals with 2D data, i.e., n_features, n_samples\n",
    "- Used to encode ‘Target variable’         - Used to encode ‘independent features’\n",
    "```\n",
    "\n",
    "_**IMPORTANT NOTE:**_\n",
    "* _Apply any feature scaling or transformation technique (such as normalization or standardization etc.) on training & testing datasets separately to prevent DATA LEAKAGE. In other words, `DO NOT apply data transformation techniques before splitting the datasets into training & testing datasets.`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac476b0-baca-4a4c-bc04-c08b467a7ea1",
   "metadata": {},
   "source": [
    "Example 1:\n",
    "* When encoding Categorical variables (ordinal or one-hot or dummy) using _`LabelEncoder`_ or _`category_encoders`_, first do it on the training dataset, then propagate to the test dataset\n",
    "  ```\n",
    "  import category_encoders as ce\n",
    "  encoder = ce.OrdinalEncoder(cols=['col1', 'col2', 'col3', 'col4', \n",
    "                                    'col5', 'col6'])\n",
    "  X_train = encoder.fit_transform(X_train)\n",
    "  X_test  = encoder.transform(X_test)     # Note, 'transform', not 'fit_transform'\n",
    "  ```\n",
    "\n",
    "Example 2:\n",
    "* When scaling the numerical variables, do the following\n",
    "  ```\n",
    "  from sklearn.preprocessing import RobustScaler\n",
    "  scaler  = RobustScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test  = scaler.transform(X_test)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740719d-3ea2-439d-aa7f-422414afd9d0",
   "metadata": {},
   "source": [
    "**Summary of Data Transforms tasks:**\n",
    "![](figures/MLPG-DataTransforms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f681e3-8b71-45c4-84d9-acb14101d236",
   "metadata": {},
   "source": [
    "Some of the Transformers provided by scikit-learn are:\n",
    "* Normalize/Feature Scaling: _`Log(), MinMaxScaler(), MAxAbsScaler(), Normalizer(), Binarizer()`_\n",
    "* Standardize: _`StandardScaler()`_\n",
    "* Robust: _`RobustScaler()`_\n",
    "* Power: _`PowerTransformer()`_\n",
    "* Quantile: _`QuantileTransformer()`_\n",
    "* Discretize: _`KBinsDiscretizer()`_\n",
    "* Polynomial: _`PolynomialFeatures()`_\n",
    "* One-Hot encode: _`OneHotEncoder()`_\n",
    "* Label binarize: _`LabelBinarizer()`_\n",
    "* Label encode: _`LabelEncoder()`_\n",
    "* Ordinal encode: _`OrdinalEncoder()`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af18e93-0e06-4658-8333-55f04a3b3379",
   "metadata": {},
   "source": [
    "### 10.2.7. Handling Imbalanced classes\n",
    "* Any real-world dataset may come with several problems and the imbalanced classes are one of them\n",
    "* The problem of imbalanced classes arises when one set of classes dominates over another set of classes\n",
    "* The former is called the majority class while the latter is called the minority class\n",
    "* This is a very common problem in machine learning where we have datasets with a disproportionate ratio of observations in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0b1f-1051-43fe-92d5-eb5469658486",
   "metadata": {},
   "source": [
    "#### 10.2.7.1. Problems with imbalanced learning\n",
    "* The problem of imbalanced classes is very common and it is bound to happen\n",
    "* The problem of learning from imbalanced data has new and modern approaches\n",
    "* This learning from imbalanced data is referred to as imbalanced learning\n",
    "* Significant problems may arise with imbalanced learning. These are as follows:\n",
    "  - It causes the machine learning model to be more biased towards the majority class\n",
    "  - It causes poor classification of minority classes. Hence, this problem throws the question of \"accuracy\" out of the question\n",
    "  - If the Imbalanced classes problem is not addressed properly, then we may end up with higher accuracy. But this higher accuracy is meaningless because it comes from a meaningless metric that is not suitable for the dataset in question. Hence, this higher accuracy no longer reliably measures model performance\n",
    "  - There may be inherent complex characteristics in the dataset. Imbalanced learning from such datasets requires new approaches, principles, tools, and techniques. But it cannot guarantee an efficient solution to the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b735bbf-4b45-4918-b4d1-bed32237ef47",
   "metadata": {},
   "source": [
    "#### 10.2.7.2. Example of imbalanced classes\n",
    "* The problem of imbalanced classes may appear in many areas including the following:\n",
    "  - Disease detection, Fraud detection, Anomaly detection\n",
    "  - Earthquake prediction, Churn prediction, Intrusion prediction\n",
    "  - Spam filtering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00a80f-3189-4d88-affe-00ebe7738482",
   "metadata": {},
   "source": [
    "#### 10.2.7.3. Approaches to handle imbalanced classes\n",
    "There are several methods to deal with the imbalanced class problems and the common ones are listed below\n",
    "* Undersampling methods \n",
    "  1. Random\n",
    "  2. Informative\n",
    "  3. NearMiss\n",
    "  4. Tomek links\n",
    "  5. Edited nearest neighbors\n",
    "  6. Cluster centroids\n",
    "* Oversampling methods \n",
    "  1. Random \n",
    "  2. Cluster-based\n",
    "  3. Synthetic data generation (SMOTE & ADASYN)\n",
    "* Other methods\n",
    "  1. Cost-sensitive learning\n",
    "  2. Algorithmic Ensemble methods\n",
    "  3. Imbalanced learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8f386-3014-4965-a245-ddd01cf725be",
   "metadata": {},
   "source": [
    "#### **Undersampling methods**\n",
    "\n",
    "The undersampling methods work with the majority class. In these methods, we randomly eliminate instances of the majority class. It reduces the number of observations from the majority class to make the dataset balanced. It results in a severe loss of information. This method is applicable when the dataset is huge and reducing the number of training samples makes the dataset balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681001b5-11c6-4ad8-9d9a-90600f0b39d4",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.1. Random undersampling\n",
    "* In the random undersampling method, we balance the imbalanced class distribution by choosing and eliminating observations from the majority class to make the dataset balanced. This approach has some pros and cons:\n",
    "* `Advantages:`\n",
    "  - If the dataset is huge, we might face run time and storage problems. Undersampling can help to handle these problems successfully by improving run time and storage problems by reducing the number of training data samples\n",
    "* `Disadvantages:`\n",
    "  - This method can discard potentially useful information that could be important for building the classifiers\n",
    "  - The sample chosen by random undersampling may be a biased one. It may not be an accurate representation of the population. So, it results in inaccurate results with the actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e44a17-5eff-448f-bc61-795f32017481",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.2. Informative undersampling\n",
    "* In informative undersampling, we follow a pre-defined selection criterion to remove the observations from the majority class\n",
    "* Within this informative undersampling technique, we have EasyEnsemble and BalanceCascade algorithms. These algorithms produce good results and are relatively easy to follow\n",
    "* `Easy ensemble:` This technique extracts several subsets of independent samples with replacements from the majority class. Then it develops multiple classifiers based on the combination of each subset with a minority class. It works just like an unsupervised learning algorithm\n",
    "* `BalanceCascade:` This method takes a supervised learning approach where it develops an ensemble of classifiers and systematically selects which majority class to the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db306103-f87c-4052-a582-944f4a2d9470",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.3. NearMiss undersampling\n",
    "* In near-miss undersampling, we only sample the data points from the majority class which is necessary to distinguish the majority class from other classes\n",
    "* `NearMiss-1:`\n",
    "  - In the NearMiss-1 sampling technique, we select samples from the majority class for which the average distance of the N closest samples of a minority class is the smallest\n",
    "* `NearMiss-2:`\n",
    "  - In the NearMiss-2 sampling technique, we select samples from the majority class for which the average distance of the N farthest samples of a minority class is the smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9eb4a-91bb-4758-93cb-ff25d48bc0ba",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.4. Tomek links undersampling\n",
    "* A Tomek's link can be defined as the set of two observations of different classes that are nearest neighbors of each other and remove these points and increase the separation gap between the two classes. Now, the algorithms produce more reliable output\n",
    "* This technique will not produce a balanced dataset. It will simply clean the dataset by removing the Tomek links\n",
    "* It may result in an easier classification problem. Thus, by removing the Tomek links, we can improve the performance of the classifier even if we don't have a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558b544-1c23-4ee0-81e3-f1a5af93d07c",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.5. Edited nearest neighbors undersampling\n",
    "* In this type of undersampling technique, we apply the nearest neighbors algorithm\n",
    "* We modify the dataset by removing samples that differ from their neighborhood. We select a subset of data to be under-sampled\n",
    "* For each sample in the subset, the nearest neighbors are computed and if the selection criteria are not fulfilled, the sample is removed\n",
    "* This technique is very much similar to Tomek’s links approach. We are not trying to achieve a class imbalance, instead, we try to remove noisy observations in the dataset to make for an easier classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526b532-931a-4970-aa07-35f2ae84de8a",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.6. Cluster centroids undersampling\n",
    "* In this technique, we perform undersampling by generating centroids based on clustering methods\n",
    "* The dataset will be grouped by similarity, to preserve information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6ce39-5df7-45b0-9802-e429512f5b68",
   "metadata": {},
   "source": [
    "#### **Oversampling methods**\n",
    "\n",
    "The Oversampling methods work with the minority class. In these methods, we duplicate random instances of the minority class. So, it replicates the observations from minority classes to balance the data. It is also known as upsampling. It may result in overfitting due to duplication of data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825969ba-079e-486d-9280-74e18396361c",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.7. Random oversampling\n",
    "* In random oversampling, we balance the data by randomly oversampling the minority class\n",
    "* `Advantages:`\n",
    "  - It leads to no information loss\n",
    "  - This method outperforms undersampling\n",
    "* `Disadvantages:`\n",
    "  - This method increases the likelihood of overfitting as it replicates the minority class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4235e9-ecf6-4863-9d6b-6164109ea370",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.8. Cluster-based oversampling\n",
    "* In this method, the K-Means clustering algorithm technique is independently applied to minority and majority class labels\n",
    "* Thus, we will identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size\n",
    "* `Advantages:`\n",
    "  - This clustering technique helps to overcome the challenge of imbalanced class distribution.\n",
    "  - Also, this technique overcomes the challenges of within-class imbalance, where a class is composed of different sub-clusters and each sub-cluster does not contain the same number of examples\n",
    "* `Disadvantages:`\n",
    "  - The disadvantage associated with this technique is the possibility of overfitting the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd0a8f-a8d3-43e4-90d6-08fa4085a25f",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.9. Synthetic data generation oversampling (SMOTE & ADASYN)\n",
    "* In the synthetic data generation technique, we overcome the data imbalances by generating artificial data\n",
    "\n",
    "`Synthetic Minority Oversampling Technique or SMOTE:`\n",
    "* In the context of synthetic data generation, there is a powerful and widely used method known as `SMOTE`\n",
    "* SMOTE generates new observations by interpolation between existing observations in the dataset\n",
    "* In SMOTE, we use a pre-specified criterion and synthetically generate minority class observations\n",
    "* These synthetic instances are then added to the original dataset. The new dataset is then used as a sample to train the classification models\n",
    "* This technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset\n",
    "* Under this technique, artificial data is created based on feature space\n",
    "* Artificial data is generated with bootstrapping and the k-nearest neighbors algorithm. It works as follows:\n",
    "  1. First of all, we take the differences between the feature vector (sample) under consideration and its nearest neighbor\n",
    "  2. Then we multiply this difference by a random number between 0 and 1\n",
    "  3. Then we add this number to the feature vector under consideration\n",
    "  4. Thus, we select a random point along the line segment between two specific features\n",
    "* `Advantages:`\n",
    "  1. This technique reduces the problem of overfitting\n",
    "  2. It does not result in the loss of useful information\n",
    "* `Disadvantages:`\n",
    "  1. Generating synthetic examples SMOTE does not take into account neighboring examples from other classes. It may result in overlapping classes and can introduce additional noise\n",
    "  2. SMOTE is not very effective for high-dimensional data\n",
    "  \n",
    "_**IMPORTANT NOTE:** SMOTE resampling should NOT be done for test datasets (\"X_test\" & \"y_test\")_\n",
    "\n",
    "`Adaptive Synthetic Technique or ADASYN:`\n",
    "* This technique works similarly to SMOTE. But the number of samples generated is proportional to the number of nearby samples which do not belong to the same class\n",
    "* Thus, it focuses on outliers when generating the new training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a627093-083f-4389-a6ae-0cecb04a56ee",
   "metadata": {},
   "source": [
    "#### **Other methods**\n",
    "\n",
    "#### 10.2.7.3.10. Cost-sensitive learning\n",
    "* Cost-sensitive learning is another commonly used method to handle imbalanced classification problems\n",
    "* This method evaluates the cost associated with misclassifying the observations\n",
    "* This method does not create balanced data distribution, rather it focuses on the imbalanced learning problem by using cost matrices which describe the cost for misclassification in a particular scenario\n",
    "* Researche has shown that this cost-sensitive learning may outperform sampling methods\n",
    "* So, it provides a likely alternative to sampling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda1e34-268a-45e1-8319-0304491048da",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.11. Algorithmic ensemble methods\n",
    "* So far, we have looked at techniques to provide balanced datasets\n",
    "* In this approach, we modify the existing classification algorithms to make them appropriate for imbalanced datasets\n",
    "* In this approach, we construct several two-stage classifiers from the original data, and then we aggregate their predictions\n",
    "* The main aim of this ensemble technique is to improve the performance of single classifiers\n",
    "* The ensemble techniques are of two types: bagging and boosting. These techniques are discussed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e7699-2309-4bc9-90ce-9612eddc8947",
   "metadata": {},
   "source": [
    "**`a. Bagging`**\n",
    "* Bagging is an abbreviation of Bootstrap Aggregating\n",
    "* In the conventional bagging algorithm, we generate n different bootstrap training samples with replacement\n",
    "* Then we train the algorithm on each bootstrap training sample separately and then aggregate the predictions at the end\n",
    "* Bagging is used to reduce overfitting to create strong learners so that we can generate strong predictions\n",
    "* Bagging allows replacement in the bootstrapped training sample\n",
    "* The ML algorithms like logistic regression, decision tree, and neural networks are fitted to each bootstrapped training sample\n",
    "* These classifiers are then aggregated to produce a compound classifier\n",
    "* This ensemble technique produces a strong compound classifier since it combines individual classifiers to come up with a strong classifier\n",
    "* `Advantages:`\n",
    "  - This technique improves the stability and accuracy of ML algorithms\n",
    "  - It reduces variance and overcomes overfitting\n",
    "  - It improves the misclassification rate of the bagged classifier\n",
    "  - In noisy data situations, bagging outperforms boosting\n",
    "* `Disadvantages:`\n",
    "  - Bagging works only if the base classifiers are not bad, to begin with. Bagging with bad classifiers can further degrade the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e4b45-73d9-4b07-bdd8-fb88e8a161ea",
   "metadata": {},
   "source": [
    "**`b. Boosting`**\n",
    "* Boosting is an ensemble technique to combine weak learners to create a strong learner so that we can make accurate predictions\n",
    "* In boosting, we start with a base or weak classifier that is prepared on the training data\n",
    "* The base learners are weak. So, the prediction accuracy is only slightly better than average\n",
    "* A classifier learning algorithm is said to be weak when small changes in data result in big changes in the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cca6af-8ca1-4311-a179-b936981097f9",
   "metadata": {},
   "source": [
    "#### 10.2.7.3.12. Imbalanced learn\n",
    "* There is a Python library, called _`Imbalanced-Learn`_ that enables us to handle the imbalanced datasets\n",
    "* It is a Python library that contains various algorithms to handle imbalanced datasets\n",
    "* It can be easily installed with the pip command\n",
    "* This library contains a _`make_imbalance`_ method to exasperate the level of class imbalance within a given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65c45d-dafb-49d8-b6c3-cec0f1aba2ea",
   "metadata": {},
   "source": [
    "## 10.3. Deliverables from Stage-4\n",
    "* Preprocessed datasets\n",
    "* Metadata for preprocessed datasets\n",
    "* Preprocessed data summary report\n",
    "* Training datasets\n",
    "* Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d30e81-b128-4862-986b-04adb4b7a69d",
   "metadata": {},
   "source": [
    "## 10.4. Notebook development tips"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5570c527-4111-4c02-8cbb-d967c16e5966",
   "metadata": {},
   "source": [
    "###   DATA PREPROCESSING   ###\n",
    "\n",
    "# Pick the tasks and activities from Stage-3 and Stage-4 based on the decisions made #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2604d04-4911-4cd2-9a2f-7547114358d5",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<br>\n",
    "\n",
    "<[ [Stage-3: Research](09.00-mlpg-Stage-3-Research.ipynb) | [Contents and Acronyms](00.00-mlpg-Contents-and-Acronyms.ipynb) | [Stage-5: Model Development](11.00-mlpg-Stage-5-Model-Development.ipynb) ]>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
